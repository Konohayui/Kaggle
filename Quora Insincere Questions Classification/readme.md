## Baseline Models' Performances
![train loss](https://raw.githubusercontent.com/shenmemingzine/Kaggle/master/Quora%20Insincere%20Questions%20Classification/pipelines/Train_Loss.png)
![val loss](https://raw.githubusercontent.com/shenmemingzine/Kaggle/master/Quora%20Insincere%20Questions%20Classification/pipelines/Val_Loss.png)

## Readings:

* [Identifying Mislabeled Training Data](https://arxiv.org/pdf/1106.0219.pdf)
* [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/pdf/1106.1813.pdf)
* [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
* [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
* [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)
* [Sentence Ordering and Coherence Modeling using Recurrent Neural Networks](https://arxiv.org/pdf/1611.02654.pdf)
* [Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks](https://arxiv.org/pdf/1811.01910.pdf)
* [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/pdf/1803.05407.pdf)
* [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf)
* [Regularization of Neural Networks using DropConnect](https://cs.nyu.edu/~wanli/dropc/dropc.pdf)
* [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)
* [Is preprocessing of text really worth your time for toxic comment classification?](https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/ICA4290.pdf)

* Meta embedding

  * [Dynamic Meta-Embeddings for Improved Sentence Representations](http://aclweb.org/anthology/D18-1176)
  * [Angular-Based Word Meta-Embedding Learning](https://arxiv.org/pdf/1808.04334.pdf)
  * [Learning Meta-Embeddings by Using Ensembles of Embedding Sets](https://arxiv.org/pdf/1508.04257.pdf)
  
* Text Augmentation

  * [Text Understanding from Scratch](https://arxiv.org/pdf/1502.01710.pdf)
  * [DATA NOISING AS SMOOTHING IN NEURAL NETWORK LANGUAGE MODELS](https://arxiv.org/pdf/1703.02573.pdf))

* F1 Score
  * [Thresholding Classifiers to Maximize F1 Score](https://arxiv.org/pdf/1402.1892.pdf)
