{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, random as rn, os, gc, re, time\n",
    "start = time.time()\n",
    "\n",
    "seed = 32\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "import tensorflow as tf\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads = 1,\n",
    "                              inter_op_parallelism_threads = 1)\n",
    "tf.set_random_seed(seed) \n",
    "sess = tf.Session(graph = tf.get_default_graph(), config = session_conf)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.layers import Input, Dense, CuDNNLSTM, Bidirectional, Activation, Conv1D\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import Add, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate, SpatialDropout1D, CuDNNGRU, Lambda, GaussianDropout\n",
    "from keras.layers import PReLU, ReLU, ELU\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.initializers import he_normal, he_uniform,  glorot_normal\n",
    "from keras.initializers import glorot_uniform, zeros, orthogonal\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from lime import submodular_pick\n",
    "from collections import OrderedDict\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "embedding_file1 = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "embedding_file2 = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "\n",
    "embed_size = 300\n",
    "max_features = 100000\n",
    "max_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7fd31e732143b7e22e0cb4d6562be249c812fdfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "misspell list (quora vs. glove)\n",
    "\"\"\"\n",
    "mispell_dict = {\n",
    "        'Terroristan': 'terrorist Pakistan',\n",
    "        'terroristan': 'terrorist Pakistan',\n",
    "        'BIMARU': 'Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh',\n",
    "        'Hinduphobic': 'Hindu phobic',\n",
    "        'hinduphobic': 'Hindu phobic',\n",
    "        'Hinduphobia': 'Hindu phobic',\n",
    "        'hinduphobia': 'Hindu phobic',\n",
    "        'Babchenko': 'Arkady Arkadyevich Babchenko faked death',\n",
    "        'Boshniaks': 'Bosniaks',\n",
    "        'Dravidanadu': 'Dravida Nadu',\n",
    "        'mysoginists': 'misogynists',\n",
    "        'MGTOWS': 'Men Going Their Own Way',\n",
    "        'mongloid': 'Mongoloid',\n",
    "        'unsincere': 'insincere',\n",
    "        'meninism': 'male feminism',\n",
    "        'jewplicate': 'jewish replicate',\n",
    "        'unoin': 'Union',\n",
    "        'daesh': 'Islamic State of Iraq and the Levant',\n",
    "        'Kalergi': 'Coudenhove-Kalergi',\n",
    "        'Bhakts': 'Bhakt',\n",
    "        'bhakts': 'Bhakt',\n",
    "        'Tambrahms': 'Tamil Brahmin',\n",
    "        'Pahul': 'Amrit Sanskar',\n",
    "        'SJW': 'social justice warrior',\n",
    "        'SJWs': 'social justice warrior',\n",
    "        'incel': ' involuntary celibates',\n",
    "        'incels': ' involuntary celibates',\n",
    "        'emiratis': 'Emiratis',\n",
    "        'weatern': 'western',\n",
    "        'westernise': 'westernize',\n",
    "        'Pizzagate': 'Pizzagate conspiracy theory',\n",
    "        'naïve': 'naive',\n",
    "        'Skripal': 'Sergei Skripal',\n",
    "        'Remainers': 'British remainer',\n",
    "        'remainers': 'British remainer',\n",
    "        'bremainer': 'British remainer',\n",
    "        'antibrahmin': 'anti Brahminism',\n",
    "        'HYPSM': 'Harvard, Yale, Princeton, Stanford, MIT',\n",
    "        'HYPS': 'Harvard, Yale, Princeton, Stanford',\n",
    "        'kompromat': 'compromising material',\n",
    "        'Tharki': 'pervert',\n",
    "        'tharki': 'pervert',\n",
    "        'mastuburate': 'masturbate',\n",
    "        'Zoë': 'Zoe',\n",
    "        'indans': 'Indian',\n",
    "        'xender': 'gender',\n",
    "        'Naxali ': 'Naxalite ',\n",
    "        'Naxalities': 'Naxalites',\n",
    "        'Bathla': 'Namit Bathla',\n",
    "        'Mewani': 'Indian politician Jignesh Mevani',\n",
    "        'clichéd': 'cliche',\n",
    "        'cliché': 'cliche',\n",
    "        'clichés': 'cliche',\n",
    "        'Wjy': 'Why',\n",
    "        'Fadnavis': 'Indian politician Devendra Fadnavis',\n",
    "        'Awadesh': 'Indian engineer Awdhesh Singh',\n",
    "        'Awdhesh': 'Indian engineer Awdhesh Singh',\n",
    "        'Khalistanis': 'Sikh separatist movement',\n",
    "        'madheshi': 'Madheshi',\n",
    "        'BNBR': 'Be Nice, Be Respectful',\n",
    "        'Bolsonaro': 'Jair Bolsonaro',\n",
    "        'XXXTentacion': 'Tentacion',\n",
    "        'Padmavat': 'Indian Movie Padmaavat',\n",
    "        'Žižek': 'Slovenian philosopher Slavoj Žižek',\n",
    "        'Adityanath': 'Indian monk Yogi Adityanath',\n",
    "        'Brexit': 'British Exit',\n",
    "        'Brexiter': 'British Exit supporter',\n",
    "        'Brexiters': 'British Exit supporters',\n",
    "        'Brexiteer': 'British Exit supporter',\n",
    "        'Brexiteers': 'British Exit supporters',\n",
    "        'Brexiting': 'British Exit',\n",
    "        'Brexitosis': 'British Exit disorder',\n",
    "        'brexit': 'British Exit',\n",
    "        'brexiters': 'British Exit supporters',\n",
    "        'jallikattu': 'Jallikattu',\n",
    "        'fortnite': 'Fortnite ',\n",
    "        'Swachh': 'Swachh Bharat mission campaign ',\n",
    "        'Quorans': 'Quoran',\n",
    "        'Qoura ': 'Quora ',\n",
    "        'quoras': 'Quora',\n",
    "        'Quroa': 'Quora',\n",
    "        'QUORA': 'Quora',\n",
    "        'narcissit': 'narcissist',\n",
    "        # extra in sample\n",
    "        'Doklam': 'Tibet',\n",
    "        'Drumpf': 'Donald Trump fool',\n",
    "        'Drumpfs': 'Donald Trump fools',\n",
    "        'Strzok': 'Hillary Clinton scandal',\n",
    "        'rohingya': 'Rohingya ',\n",
    "        'wumao': 'cheap Chinese stuff',\n",
    "        'wumaos': 'cheap Chinese stuff',\n",
    "        'Sanghis': 'Sanghi',\n",
    "        'Tamilans': 'Tamils',\n",
    "        'biharis': 'Biharis',\n",
    "        'Rejuvalex': 'hair growth formula',\n",
    "        'Feku': 'Fake',\n",
    "        'deplorables': 'deplorable',\n",
    "        'muhajirs': 'Muslim immigrant',\n",
    "        'Gujratis': 'Gujarati',\n",
    "        'Chutiya': 'Fucker',\n",
    "        'Chutiyas': 'Fucker',\n",
    "        'thighing': 'masturbate',\n",
    "        '卐': 'Nazi Germany',\n",
    "        'Pribumi': 'Native Indonesian',\n",
    "        'Gurmehar': 'Gurmehar Kaur Indian student activist',\n",
    "        'Novichok': 'Soviet Union agents',\n",
    "        'Khazari': 'Khazars',\n",
    "        'Demonetization': 'demonetization',\n",
    "        'demonetisation': 'demonetization',\n",
    "        'demonitisation': 'demonetization',\n",
    "        'demonitization': 'demonetization',\n",
    "        'demonetisation': 'demonetization',\n",
    "        'cryptocurrencies': 'cryptocurrency',\n",
    "        'Hindians': 'North Indian who hate British',\n",
    "        'vaxxer': 'vocal nationalist ',\n",
    "        'remoaner': 'remainer ',\n",
    "        'bremoaner': 'British remainer ',\n",
    "        'Jewism': 'Judaism',\n",
    "        'Eroupian': 'European',\n",
    "        'WMAF': 'White male married Asian female',\n",
    "        'moeslim': 'Muslim',\n",
    "        'cishet': 'cisgender and heterosexual person',\n",
    "        'Eurocentric': 'Eurocentrism ',\n",
    "        'Jewdar': 'Jew dar',\n",
    "        'Asifa': 'abduction, rape, murder case ',\n",
    "        'marathis': 'Marathi',\n",
    "        'Trumpanzees': 'Trump chimpanzee fool',\n",
    "        'Crimean': 'Crimea people ',\n",
    "        'atrracted': 'attract',\n",
    "        'LGBT': 'lesbian, gay, bisexual, transgender',\n",
    "        'Boshniak': 'Bosniaks ',\n",
    "        'Myeshia': 'widow of Green Beret killed in Niger',\n",
    "        'demcoratic': 'Democratic',\n",
    "        'raaping': 'rape',\n",
    "        'Dönmeh': 'Islam',\n",
    "        'feminazism': 'feminism nazi',\n",
    "        'langague': 'language',\n",
    "        'Hongkongese': 'HongKong people',\n",
    "        'hongkongese': 'HongKong people',\n",
    "        'Kashmirians': 'Kashmirian',\n",
    "        'Chodu': 'fucker',\n",
    "        'penish': 'penis',\n",
    "        'micropenis': 'tiny penis',\n",
    "        'Madridiots': 'Real Madrid idiot supporters',\n",
    "        'Ambedkarite': 'Dalit Buddhist movement ',\n",
    "        'ReleaseTheMemo': 'cry for the right and Trump supporters',\n",
    "        'harrase': 'harass',\n",
    "        'Barracoon': 'Black slave',\n",
    "        'Castrater': 'castration',\n",
    "        'castrater': 'castration',\n",
    "        'Rapistan': 'Pakistan rapist',\n",
    "        'rapistan': 'Pakistan rapist',\n",
    "        'Turkified': 'Turkification',\n",
    "        'turkified': 'Turkification',\n",
    "        'Dumbassistan': 'dumb ass Pakistan',\n",
    "        'facetards': 'Facebook retards',\n",
    "        'rapefugees': 'rapist refugee',\n",
    "        'superficious': 'superficial',\n",
    "        # extra from kagglers\n",
    "        'colour': 'color',\n",
    "        'centre': 'center',\n",
    "        'favourite': 'favorite',\n",
    "        'travelling': 'traveling',\n",
    "        'counselling': 'counseling',\n",
    "        'theatre': 'theater',\n",
    "        'cancelled': 'canceled',\n",
    "        'labour': 'labor',\n",
    "        'organisation': 'organization',\n",
    "        'wwii': 'world war 2',\n",
    "        'citicise': 'criticize',\n",
    "        'youtu ': 'youtube ',\n",
    "        'sallary': 'salary',\n",
    "        'Whta': 'What',\n",
    "        'narcisist': 'narcissist',\n",
    "        'narcissit': 'narcissist',\n",
    "        'howdo': 'how do',\n",
    "        'whatare': 'what are',\n",
    "        'howcan': 'how can',\n",
    "        'howmuch': 'how much',\n",
    "        'howmany': 'how many',\n",
    "        'whydo': 'why do',\n",
    "        'doI': 'do I',\n",
    "        'theBest': 'the best',\n",
    "        'howdoes': 'how does',\n",
    "        'mastrubation': 'masturbation',\n",
    "        'mastrubate': 'masturbate',\n",
    "        'mastrubating': 'masturbating',\n",
    "        'pennis': 'penis',\n",
    "        'Etherium': 'Ethereum',\n",
    "        'bigdata': 'big data',\n",
    "        '2k17': '2017',\n",
    "        '2k18': '2018',\n",
    "        'qouta': 'quota',\n",
    "        'exboyfriend': 'ex boyfriend',\n",
    "        'airhostess': 'air hostess',\n",
    "        'whst': 'what',\n",
    "        'watsapp': 'whatsapp',\n",
    "        # extra\n",
    "        'bodyshame': 'body shaming',\n",
    "        'bodyshoppers': 'body shopping',\n",
    "        'bodycams': 'body cams',\n",
    "        'Cananybody': 'Can any body',\n",
    "        'deadbody': 'dead body',\n",
    "        'deaddict': 'de addict',\n",
    "        'Northindian': 'North Indian ',\n",
    "        'northindian': 'north Indian ',\n",
    "        'northkorea': 'North Korea',\n",
    "        'Whykorean': 'Why Korean',\n",
    "        'koreaboo': 'Korea boo ',\n",
    "        'Brexshit': 'British Exit bullshit',\n",
    "        'shithole': 'shithole ',\n",
    "        'shitpost': 'shit post',\n",
    "        'shitslam': 'shit Islam',\n",
    "        'shitlords': 'shit lords',\n",
    "        'Fck': 'Fuck',\n",
    "        'fck': 'fuck',\n",
    "        'Clickbait': 'click bait ',\n",
    "        'clickbait': 'click bait ',\n",
    "        'mailbait': 'mail bait',\n",
    "        'healhtcare': 'healthcare',\n",
    "        'trollbots': 'troll bots',\n",
    "        'trollled': 'trolled',\n",
    "        'trollimg': 'trolling',\n",
    "        'cybertrolling': 'cyber trolling',\n",
    "        'sickular': 'India sick secular ',\n",
    "        'suckimg': 'sucking',\n",
    "        'Idiotism': 'idiotism',\n",
    "        'Niggerism': 'Nigger',\n",
    "        'Niggeriah': 'Nigger'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def spacing_misspell(text):\n",
    "    \"\"\"\n",
    "    'deadbody' -> 'dead body'\n",
    "    \"\"\"\n",
    "    misspell_list = [\n",
    "        '(F|f)uck',\n",
    "        'Trump',\n",
    "        '\\W(A|a)nti',\n",
    "        '(W|w)hy',\n",
    "        '(W|w)hat',\n",
    "        'How',\n",
    "        'care\\W',\n",
    "        '\\Wover',\n",
    "        'gender',\n",
    "        'people',\n",
    "    ]\n",
    "    misspell_re = re.compile('(%s)' % '|'.join(misspell_list))\n",
    "    return misspell_re.sub(r\" \\1 \", text)\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', \n",
    "          '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "          '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  \n",
    "          '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”', \n",
    "          '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', \n",
    "          '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', \n",
    "          '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', \n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', \n",
    "          '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', \n",
    "          '¹', '≤', '‡', '√', 'β', 'α', '∅', 'θ', '÷', '₹']\n",
    "\n",
    "def space_punct(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: space_punct(x))\n",
    "print(\"Text cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3f2bd861f6b2e5ea7c153a685bc5fdcfd560e439"
   },
   "outputs": [],
   "source": [
    "def add_num_features(data):\n",
    "    data[\"total_length\"] = data[\"question_text\"].apply(len)\n",
    "    data[\"num_words\"] = data[\"question_text\"].str.count(\"\\S+\")\n",
    "    data[\"total_length\"] = data[\"total_length\"].fillna(0)\n",
    "    data[\"num_words\"] = data[\"num_words\"].fillna(0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scale_num_features(tr, val, te):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(tr)\n",
    "    \n",
    "    tr = scaler.transform(tr)\n",
    "    val = scaler.transform(val)\n",
    "    te = scaler.transform(te)\n",
    "    \n",
    "    return tr, val, te\n",
    "\n",
    "# train = add_num_features(train)\n",
    "# test = add_num_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "01611b87da04965139ca9569f37a6e68d3ee1f4e"
   },
   "outputs": [],
   "source": [
    "def get_glove(embedding_file):\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file))\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "def get_para(embedding_file):\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file, \n",
    "                                                                   encoding=\"utf8\", \n",
    "                                                                   errors='ignore') if len(o)>100)\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "glove_index = get_glove(embedding_file1)\n",
    "# para_index = get_para(embedding_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "72be9cec7349ad80dd10cb3fe8eb0b2cc7f0566d"
   },
   "outputs": [],
   "source": [
    "def get_embed(tokenizer = None, embeddings_index = None, emb_mean = None, emb_std = None):\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return nb_words, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f49136f57e05cc5613f7ed758ce41192122554f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = max_features, \n",
    "                      filters = '\"#$%&()*+/:;<=>@[\\]^_`{|}~',\n",
    "                      lower = False)\n",
    "tokenizer.fit_on_texts(train[\"question_text\"])\n",
    "\n",
    "train_token = tokenizer.texts_to_sequences(train[\"question_text\"])\n",
    "train_seq = pad_sequences(train_token, maxlen = max_len)\n",
    "del train_token; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9b8d27d9d4c55bfded75eff598679aa364316480"
   },
   "outputs": [],
   "source": [
    "target = train[\"target\"].values\n",
    "# sincere = train[train[\"target\"] == 0][\"qid\"]\n",
    "# insincere = train[train[\"target\"] == 1][\"qid\"]\n",
    "\n",
    "# n_sincere = len(sincere)\n",
    "# n_insincere = len(insincere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a8da8aa77eed1e3c08a124a30b6ef2575637d5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix completed!\n"
     ]
    }
   ],
   "source": [
    "nb_words, embedding_matrix1 = get_embed(tokenizer = tokenizer, embeddings_index = glove_index, \n",
    "                                        emb_mean = -0.005838499, \n",
    "                                        emb_std = 0.48782197)\n",
    "# nb_words, embedding_matrix2 = get_embed(tokenizer = tokenizer, embeddings_index = para_index, \n",
    "#                                         emb_mean = -0.0053247833, \n",
    "#                                         emb_std = 0.49346462)\n",
    "# embedding_matrix = np.mean([embedding_matrix1, embedding_matrix2], axis = 0)\n",
    "# del embedding_matrix1, embedding_matrix2; gc.collect()\n",
    "print(\"Embedding matrix completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8f94f27cbc2f20f759973fc883846a603b099c88"
   },
   "outputs": [],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import K\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "40497a4460bf58d428f1ecf0a16b84f69b30e69f"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "228e39d82d435a0a3c3a6e042a812d280a029f2a"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "216bfd88758d2375920633a23bda0f3173709826"
   },
   "outputs": [],
   "source": [
    "class SWA(Callback):\n",
    "    \n",
    "    def __init__(self, filepath, swa_epoch):\n",
    "        super(SWA, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.swa_epoch = swa_epoch \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        print('Stochastic weight averaging selected for last {} epochs.'\n",
    "              .format(self.nb_epoch - self.swa_epoch))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch == self.swa_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "            \n",
    "        elif epoch > self.swa_epoch:    \n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.swa_weights[i] = (self.swa_weights[i] * \n",
    "                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "        print('Final model parameters set to stochastic weight average.')\n",
    "        self.model.save_weights(self.filepath)\n",
    "        print('Final stochastic averaged weights saved to file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "79f4b519a61b3bb860d53abf6fcee34de7a315c4"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "import six\n",
    "import copy\n",
    "from six.moves import zip\n",
    "from keras.utils.generic_utils import serialize_keras_object\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Decoupled weight decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [Optimization for Deep Learning Highlights in 2017](http://ruder.io/deep-learning-optimization-2017/index.html)\n",
    "        - [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/6)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.init_lr = lr # decoupled weight decay (2/6)\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (3/6)\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd # decoupled weight decay (4/6)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "        eta_t = lr / self.init_lr # decoupled weight decay (5/6)\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - eta_t * wd * p # decoupled weight decay (6/6)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "30f840cd23fee49c4ebe420dad4558866dfc318b"
   },
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    k-max-pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "14859dc6d8ee8ecf4da88a5da2d7ea08a8e69c87"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Wrapper\n",
    "import keras.backend as K\n",
    "\n",
    "class DropConnect(Wrapper):\n",
    "    def __init__(self, layer, prob=1., **kwargs):\n",
    "        self.prob = prob\n",
    "        self.layer = layer\n",
    "        super(DropConnect, self).__init__(layer, **kwargs)\n",
    "        if 0. < self.prob < 1.:\n",
    "            self.uses_learning_phase = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(DropConnect, self).build()\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        if 0. < self.prob < 1.:\n",
    "            self.layer.kernel = K.in_train_phase(K.dropout(self.layer.kernel, self.prob), self.layer.kernel)\n",
    "            self.layer.bias = K.in_train_phase(K.dropout(self.layer.bias, self.prob), self.layer.bias)\n",
    "        return self.layer.call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "603a5fbd909dbf5f0c1fb9d1f1b3182035d6c218"
   },
   "outputs": [],
   "source": [
    "def get_f1(true, val):\n",
    "    precision, recall, thresholds = precision_recall_curve(true, val)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_threshold = thresholds[np.argmax(F)]\n",
    "    \n",
    "    return best_threshold, best_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4cc828b95dd8d1fbf8d2969ccf3e7a03cd5fb8d9"
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(nb_words, embedding_matrix1, \n",
    "#                      num_feat,\n",
    "                     units = 40, dr = 0.3, \n",
    "                     num_capsules = 10, dim_capsules = 10, routs = 4,\n",
    "                     _k = 2):\n",
    "    \n",
    "    inp = Input(shape = (max_len, ))\n",
    "#     num_inp = Input(shape = (num_feat.shape[1], ))\n",
    "    embed_layer = Embedding(nb_words, embed_size, input_length = max_len,\n",
    "                             weights = [embedding_matrix1], trainable = False)(inp)\n",
    "    x = SpatialDropout1D(dr, seed = seed)(embed_layer)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(units, kernel_initializer = glorot_normal(seed = seed), \n",
    "                                recurrent_initializer = orthogonal(gain = 1.0, seed = seed), \n",
    "                                return_sequences = True))(x)\n",
    "\n",
    "    x = Capsule(num_capsule = num_capsules, dim_capsule = dim_capsules, routings = routs)(x)\n",
    "    x, x_h, x_c = Bidirectional(CuDNNGRU(units, kernel_initializer = glorot_normal(seed = seed),\n",
    "                                         recurrent_initializer = orthogonal(gain = 1.0, seed = seed),\n",
    "                                         return_sequences = True, return_state = True))(x)\n",
    "    att = Attention(num_capsules)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "#     max_pool = KMaxPooling(k = _k)(x)\n",
    "    \n",
    "    main = concatenate([x_h, att, avg_pool, max_pool])\n",
    "#     main = Dense(256, kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "#     main = Activation(\"relu\")(main)\n",
    "#     main = Dropout(0.2, seed = seed)(main)\n",
    "    main = DropConnect(Dense(256, kernel_initializer = glorot_normal(seed = seed)), 0.1)(main)\n",
    "#     main = Dense(64, kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "    main = Activation(\"relu\")(main)\n",
    "#     main = Dropout(0.1, seed = seed)(main)\n",
    "    \n",
    "#     main = Dense(32, kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "#     main = Activation(\"relu\")(main)\n",
    "#     main = Dropout(0.05, seed = seed)(main)\n",
    "    \n",
    "    out = Dense(1, activation = \"sigmoid\", \n",
    "                kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    model.compile(loss = \"binary_crossentropy\",\n",
    "                  optimizer = AdamW(lr = 2e-3, weight_decay = 0.5e-4), \n",
    "                  metrics = None)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "084ad4851b73da151fe9fd52ef61296c9093477e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Training 1/5 fold\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/9\n",
      " - 137s - loss: 0.1311 - val_loss: 0.1125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11254, saving model to fold_1_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1092 - val_loss: 0.1037\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11254 to 0.10370, saving model to fold_1_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1037 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10370 to 0.10139, saving model to fold_1_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1003 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10139 to 0.09896, saving model to fold_1_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0971 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09896 to 0.09814, saving model to fold_1_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0940 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09814 to 0.09737, saving model to fold_1_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0910 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09737 to 0.09580, saving model to fold_1_best_model.h5\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0886 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09580\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0874 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09580\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.35794901847839355 is 0.6946725986767055.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 2/5 fold\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/9\n",
      " - 134s - loss: 0.1311 - val_loss: 0.1093\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10935, saving model to fold_2_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1089 - val_loss: 0.1028\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10935 to 0.10281, saving model to fold_2_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1038 - val_loss: 0.1006\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10281 to 0.10057, saving model to fold_2_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1004 - val_loss: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10057 to 0.09995, saving model to fold_2_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0971 - val_loss: 0.0987\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09995 to 0.09870, saving model to fold_2_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0939 - val_loss: 0.0973\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09870 to 0.09726, saving model to fold_2_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0911 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09726\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0890 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09726 to 0.09637, saving model to fold_2_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0876 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09637\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n",
      "F1 score at threshold 0.3382054269313812 is 0.6920457169244979.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 3/5 fold\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/9\n",
      " - 134s - loss: 0.1316 - val_loss: 0.1064\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10643, saving model to fold_3_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1090 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10643 to 0.10139, saving model to fold_3_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1040 - val_loss: 0.0997\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10139 to 0.09967, saving model to fold_3_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1006 - val_loss: 0.0980\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09967 to 0.09797, saving model to fold_3_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0976 - val_loss: 0.0963\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09797 to 0.09628, saving model to fold_3_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0944 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09628 to 0.09465, saving model to fold_3_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0914 - val_loss: 0.0942\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09465 to 0.09424, saving model to fold_3_best_model.h5\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0890 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09424 to 0.09398, saving model to fold_3_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0876 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09398\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n",
      "F1 score at threshold 0.36107274889945984 is 0.6950468848875337.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 4/5 fold\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/9\n",
      " - 133s - loss: 0.1312 - val_loss: 0.1112\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11117, saving model to fold_4_best_model.h5\n",
      "Epoch 2/9\n",
      " - 130s - loss: 0.1089 - val_loss: 0.1022\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11117 to 0.10217, saving model to fold_4_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1036 - val_loss: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10217 to 0.09999, saving model to fold_4_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1002 - val_loss: 0.0983\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09999 to 0.09830, saving model to fold_4_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0974 - val_loss: 0.0969\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09830 to 0.09686, saving model to fold_4_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0939 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09686 to 0.09614, saving model to fold_4_best_model.h5\n",
      "Epoch 7/9\n",
      " - 132s - loss: 0.0911 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09614 to 0.09591, saving model to fold_4_best_model.h5\n",
      "Epoch 8/9\n",
      " - 132s - loss: 0.0887 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09591 to 0.09518, saving model to fold_4_best_model.h5\n",
      "Epoch 9/9\n",
      " - 132s - loss: 0.0872 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09518\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n",
      "F1 score at threshold 0.31162044405937195 is 0.6929922454001775.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 5/5 fold\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Stochastic weight averaging selected for last 3 epochs.\n",
      "Epoch 1/9\n",
      " - 134s - loss: 0.1310 - val_loss: 0.1087\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10870, saving model to fold_5_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1086 - val_loss: 0.1044\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10870 to 0.10435, saving model to fold_5_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1033 - val_loss: 0.1018\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10435 to 0.10178, saving model to fold_5_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1000 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10178 to 0.09917, saving model to fold_5_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0971 - val_loss: 0.0986\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09917 to 0.09856, saving model to fold_5_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0937 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09856 to 0.09616, saving model to fold_5_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0910 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09616\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0886 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09616 to 0.09524, saving model to fold_5_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0870 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09524\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n",
      "F1 score at threshold 0.3886062204837799 is 0.6936833082084478.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold = 5\n",
    "batch_size = 1024\n",
    "epochs = 9\n",
    "units, dr = 64, 0.3\n",
    "num_capsules = 10\n",
    "dim_capsules = 10\n",
    "routs = 3\n",
    "k = 3\n",
    "\n",
    "oof_pred = np.zeros((len(target), 1))\n",
    "thresholds = [0]*fold\n",
    "kfold = StratifiedKFold(n_splits = fold, shuffle = True, random_state = seed)\n",
    "fold_idx = kfold.split(train_seq, target)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(fold_idx):\n",
    "    f = i+1\n",
    "    print(\"-\"*70)\n",
    "    print(\"Training {}/{} fold\".format(i+1, fold))\n",
    "    \n",
    "#     train_idx = pd.concat([sincere[:int(n_sincere*(i/fold))], \n",
    "#                            sincere[int(n_sincere*(f/fold)):],\n",
    "#                            insincere[:int(n_insincere*(i/fold))], \n",
    "#                            insincere[int(n_insincere*(f/fold)):]]).index\n",
    "#     val_idx = pd.concat([sincere[int(n_sincere*(i/fold)):int(n_sincere*(f/fold))],\n",
    "#                          insincere[int(n_insincere*(i/fold)):int(n_insincere*(f/fold))]]).index\n",
    "    \n",
    "    X_train, y_train = train_seq[train_idx], target[train_idx]\n",
    "    X_val, y_val = train_seq[val_idx], target[val_idx]\n",
    "    \n",
    "#     test_num = test[[\"total_length\", \"num_words\"]]\n",
    "#     train_num, val_num, test_num = scale_num_features(train_num, val_num, test_num)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    train_idx = np.random.permutation(len(X_train))\n",
    "#     val_idx = np.random.permutation(len(X_val))\n",
    "\n",
    "    X_train = X_train[train_idx]\n",
    "#     X_val = X_val[val_idx]\n",
    "#     train_num = train_num[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "#     y_val = y_val[val_idx]\n",
    "\n",
    "    best_model = \"fold_{}_best_model.h5\".format(f)\n",
    "    swa_model = \"fold_{}_swa_model.h5\".format(f)\n",
    "    check_point = ModelCheckpoint(best_model, monitor = \"val_loss\", mode = \"min\",\n",
    "                                  save_best_only = True, \n",
    "                                  save_weights_only = True,\n",
    "                                  verbose = 1)\n",
    "    clr = CyclicLR(base_lr = 0.0008, max_lr = 0.0025,\n",
    "                   step_size = 4*int(len(X_train)/batch_size), \n",
    "                   mode = \"exp_range\",\n",
    "                   gamma = 0.99994)\n",
    "    swa = SWA(swa_model, 6)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_lstm_model(nb_words, embedding_matrix1,\n",
    "                             units = units, dr = dr,\n",
    "                             num_capsules = num_capsules, dim_capsules = dim_capsules, \n",
    "                             routs = routs,  _k = k)\n",
    "    \n",
    "    lstm_hist = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n",
    "                          validation_data = (X_val, y_val), \n",
    "                          callbacks = [check_point,\n",
    "                                       swa,\n",
    "                                       clr],\n",
    "                          verbose = 2)\n",
    "\n",
    "    model.load_weights(swa_model)\n",
    "    val_pred = model.predict(X_val, batch_size = batch_size, verbose = 2)\n",
    "    threshold, score = get_f1(y_val, val_pred)\n",
    "    oof_pred[val_idx] = val_pred\n",
    "    thresholds[i] = threshold\n",
    "    \n",
    "    print(\"F1 score at threshold {} is {}.\\n\".format(threshold, score))\n",
    "    del model; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "e80daa13768b3ed2dddc268ced4a1eaf21f3cda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds of each fold: \n",
      "[0.35794901847839355, 0.3382054269313812, 0.36107274889945984, 0.31162044405937195, 0.3886062204837799]\n",
      "F1 score at average threshold 0.35373005270957947 is 0.693188395730592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"Thresholds of each fold: \\n{}\".format(thresholds))\n",
    "threshold, score = get_f1(target, oof_pred)\n",
    "print(\"F1 score at average threshold {} is {}\".format(threshold, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "fab7ba03f0bec61bc331f9fb22149b02b5b408bc"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/test.csv\").fillna(\"missing\")\n",
    "\n",
    "num_test = test.shape[0]\n",
    "pred = np.zeros((num_test, 1))\n",
    "\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: space_punct(x))\n",
    "\n",
    "test_token = tokenizer.texts_to_sequences(test[\"question_text\"])\n",
    "X_test = pad_sequences(test_token, maxlen = max_len)\n",
    "del test_token; gc.collect()\n",
    "\n",
    "for f in range(1, 6):\n",
    "    model = build_lstm_model(nb_words, embedding_matrix1,\n",
    "                             units = units, dr = dr,\n",
    "                             num_capsules = num_capsules, dim_capsules = dim_capsules, \n",
    "                             routs = routs, _k = k)\n",
    "    model.load_weights(\"fold_{}_swa_model.h5\".format(f))\n",
    "    pred += model.predict(X_test, batch_size = batch_size, verbose = 2)\n",
    "    \n",
    "pred = pred/fold\n",
    "pred = (pred > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d6678ee944dc79788a852232d46eec480131cdd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00014894849d00ba98a9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000156468431f09b3cae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000227734433360e1aae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005e06fbe3045bd2a92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00068a0f7f41f50fc399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  prediction\n",
       "0  00014894849d00ba98a9           0\n",
       "1  000156468431f09b3cae           0\n",
       "2  000227734433360e1aae           0\n",
       "3  0005e06fbe3045bd2a92           0\n",
       "4  00068a0f7f41f50fc399           0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"prediction\"] = pred\n",
    "test[[\"qid\", \"prediction\"]].to_csv(\"submission.csv\", index = False)\n",
    "test[[\"qid\", \"prediction\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "08cea335ccc596828d4bd8ea1e90824028378b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Predict completed in 6393.374658584595\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Train and Predict completed in {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "573f7a511c003c442ab08af400024fcd704ae826"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
