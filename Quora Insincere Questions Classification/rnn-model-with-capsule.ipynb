{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, random as rn, os, gc, re, time\n",
    "start = time.time()\n",
    "seed = 32\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "import tensorflow as tf\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads = 1,\n",
    "                              inter_op_parallelism_threads = 1)\n",
    "tf.set_random_seed(seed)\n",
    "sess = tf.Session(graph = tf.get_default_graph(), config = session_conf)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.layers import Input, Dense, CuDNNLSTM, Bidirectional, Activation, Conv1D\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, AlphaDropout\n",
    "from keras.layers import Add, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate, SpatialDropout1D, CuDNNGRU, Lambda, GaussianDropout, GaussianNoise\n",
    "from keras.layers import PReLU, ReLU, ELU\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.initializers import he_normal, he_uniform, glorot_normal\n",
    "from keras.initializers import glorot_uniform, zeros, orthogonal\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\").fillna(\"missing\")\n",
    "\n",
    "embedding_file1 = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "embedding_file2 = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "\n",
    "embed_size = 300\n",
    "max_features = 100000\n",
    "max_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "misspell list (quora vs. glove)\n",
    "\"\"\"\n",
    "mispell_dict = {\n",
    "        'Terroristan': 'terrorist Pakistan',\n",
    "        'terroristan': 'terrorist Pakistan',\n",
    "        'BIMARU': 'Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh',\n",
    "        'Hinduphobic': 'Hindu phobic',\n",
    "        'hinduphobic': 'Hindu phobic',\n",
    "        'Hinduphobia': 'Hindu phobic',\n",
    "        'hinduphobia': 'Hindu phobic',\n",
    "        'Babchenko': 'Arkady Arkadyevich Babchenko faked death',\n",
    "        'Boshniaks': 'Bosniaks',\n",
    "        'Dravidanadu': 'Dravida Nadu',\n",
    "        'mysoginists': 'misogynists',\n",
    "        'MGTOWS': 'Men Going Their Own Way',\n",
    "        'mongloid': 'Mongoloid',\n",
    "        'unsincere': 'insincere',\n",
    "        'meninism': 'male feminism',\n",
    "        'jewplicate': 'jewish replicate',\n",
    "        'unoin': 'Union',\n",
    "        'daesh': 'Islamic State of Iraq and the Levant',\n",
    "        'Kalergi': 'Coudenhove-Kalergi',\n",
    "        'Bhakts': 'Bhakt',\n",
    "        'bhakts': 'Bhakt',\n",
    "        'Tambrahms': 'Tamil Brahmin',\n",
    "        'Pahul': 'Amrit Sanskar',\n",
    "        'SJW': 'social justice warrior',\n",
    "        'SJWs': 'social justice warrior',\n",
    "        'incel': ' involuntary celibates',\n",
    "        'incels': ' involuntary celibates',\n",
    "        'emiratis': 'Emiratis',\n",
    "        'weatern': 'western',\n",
    "        'westernise': 'westernize',\n",
    "        'Pizzagate': 'Pizzagate conspiracy theory',\n",
    "        'naïve': 'naive',\n",
    "        'Skripal': 'Sergei Skripal',\n",
    "        'Remainers': 'British remainer',\n",
    "        'remainers': 'British remainer',\n",
    "        'bremainer': 'British remainer',\n",
    "        'antibrahmin': 'anti Brahminism',\n",
    "        'HYPSM': 'Harvard, Yale, Princeton, Stanford, MIT',\n",
    "        'HYPS': 'Harvard, Yale, Princeton, Stanford',\n",
    "        'kompromat': 'compromising material',\n",
    "        'Tharki': 'pervert',\n",
    "        'tharki': 'pervert',\n",
    "        'mastuburate': 'masturbate',\n",
    "        'Zoë': 'Zoe',\n",
    "        'indans': 'Indian',\n",
    "        'xender': 'gender',\n",
    "        'Naxali ': 'Naxalite ',\n",
    "        'Naxalities': 'Naxalites',\n",
    "        'Bathla': 'Namit Bathla',\n",
    "        'Mewani': 'Indian politician Jignesh Mevani',\n",
    "        'clichéd': 'cliche',\n",
    "        'cliché': 'cliche',\n",
    "        'clichés': 'cliche',\n",
    "        'Wjy': 'Why',\n",
    "        'Fadnavis': 'Indian politician Devendra Fadnavis',\n",
    "        'Awadesh': 'Indian engineer Awdhesh Singh',\n",
    "        'Awdhesh': 'Indian engineer Awdhesh Singh',\n",
    "        'Khalistanis': 'Sikh separatist movement',\n",
    "        'madheshi': 'Madheshi',\n",
    "        'BNBR': 'Be Nice, Be Respectful',\n",
    "        'Bolsonaro': 'Jair Bolsonaro',\n",
    "        'XXXTentacion': 'Tentacion',\n",
    "        'Padmavat': 'Indian Movie Padmaavat',\n",
    "        'Žižek': 'Slovenian philosopher Slavoj Žižek',\n",
    "        'Adityanath': 'Indian monk Yogi Adityanath',\n",
    "        'Brexit': 'British Exit',\n",
    "        'Brexiter': 'British Exit supporter',\n",
    "        'Brexiters': 'British Exit supporters',\n",
    "        'Brexiteer': 'British Exit supporter',\n",
    "        'Brexiteers': 'British Exit supporters',\n",
    "        'Brexiting': 'British Exit',\n",
    "        'Brexitosis': 'British Exit disorder',\n",
    "        'brexit': 'British Exit',\n",
    "        'brexiters': 'British Exit supporters',\n",
    "        'jallikattu': 'Jallikattu',\n",
    "        'fortnite': 'Fortnite ',\n",
    "        'Swachh': 'Swachh Bharat mission campaign ',\n",
    "        'Quorans': 'Quoran',\n",
    "        'Qoura ': 'Quora ',\n",
    "        'quoras': 'Quora',\n",
    "        'Quroa': 'Quora',\n",
    "        'QUORA': 'Quora',\n",
    "        'narcissit': 'narcissist',\n",
    "        # extra in sample\n",
    "        'Doklam': 'Tibet',\n",
    "        'Drumpf': 'Donald Trump fool',\n",
    "        'Drumpfs': 'Donald Trump fools',\n",
    "        'Strzok': 'Hillary Clinton scandal',\n",
    "        'rohingya': 'Rohingya ',\n",
    "        'wumao': 'cheap Chinese stuff',\n",
    "        'wumaos': 'cheap Chinese stuff',\n",
    "        'Sanghis': 'Sanghi',\n",
    "        'Tamilans': 'Tamils',\n",
    "        'biharis': 'Biharis',\n",
    "        'Rejuvalex': 'hair growth formula',\n",
    "        'Feku': 'Fake',\n",
    "        'deplorables': 'deplorable',\n",
    "        'muhajirs': 'Muslim immigrant',\n",
    "        'Gujratis': 'Gujarati',\n",
    "        'Chutiya': 'Fucker',\n",
    "        'Chutiyas': 'Fucker',\n",
    "        'thighing': 'masturbate',\n",
    "        '卐': 'Nazi Germany',\n",
    "        'Pribumi': 'Native Indonesian',\n",
    "        'Gurmehar': 'Gurmehar Kaur Indian student activist',\n",
    "        'Novichok': 'Soviet Union agents',\n",
    "        'Khazari': 'Khazars',\n",
    "        'Demonetization': 'demonetization',\n",
    "        'demonetisation': 'demonetization',\n",
    "        'demonitisation': 'demonetization',\n",
    "        'demonitization': 'demonetization',\n",
    "        'demonetisation': 'demonetization',\n",
    "        'cryptocurrencies': 'cryptocurrency',\n",
    "        'Hindians': 'North Indian who hate British',\n",
    "        'vaxxer': 'vocal nationalist ',\n",
    "        'remoaner': 'remainer ',\n",
    "        'bremoaner': 'British remainer ',\n",
    "        'Jewism': 'Judaism',\n",
    "        'Eroupian': 'European',\n",
    "        'WMAF': 'White male married Asian female',\n",
    "        'moeslim': 'Muslim',\n",
    "        'cishet': 'cisgender and heterosexual person',\n",
    "        'Eurocentric': 'Eurocentrism ',\n",
    "        'Jewdar': 'Jew dar',\n",
    "        'Asifa': 'abduction, rape, murder case ',\n",
    "        'marathis': 'Marathi',\n",
    "        'Trumpanzees': 'Trump chimpanzee fool',\n",
    "        'Crimean': 'Crimea people ',\n",
    "        'atrracted': 'attract',\n",
    "        'LGBT': 'lesbian, gay, bisexual, transgender',\n",
    "        'Boshniak': 'Bosniaks ',\n",
    "        'Myeshia': 'widow of Green Beret killed in Niger',\n",
    "        'demcoratic': 'Democratic',\n",
    "        'raaping': 'rape',\n",
    "        'Dönmeh': 'Islam',\n",
    "        'feminazism': 'feminism nazi',\n",
    "        'langague': 'language',\n",
    "        'Hongkongese': 'HongKong people',\n",
    "        'hongkongese': 'HongKong people',\n",
    "        'Kashmirians': 'Kashmirian',\n",
    "        'Chodu': 'fucker',\n",
    "        'penish': 'penis',\n",
    "        'micropenis': 'tiny penis',\n",
    "        'Madridiots': 'Real Madrid idiot supporters',\n",
    "        'Ambedkarite': 'Dalit Buddhist movement ',\n",
    "        'ReleaseTheMemo': 'cry for the right and Trump supporters',\n",
    "        'harrase': 'harass',\n",
    "        'Barracoon': 'Black slave',\n",
    "        'Castrater': 'castration',\n",
    "        'castrater': 'castration',\n",
    "        'Rapistan': 'Pakistan rapist',\n",
    "        'rapistan': 'Pakistan rapist',\n",
    "        'Turkified': 'Turkification',\n",
    "        'turkified': 'Turkification',\n",
    "        'Dumbassistan': 'dumb ass Pakistan',\n",
    "        'facetards': 'Facebook retards',\n",
    "        'rapefugees': 'rapist refugee',\n",
    "        'superficious': 'superficial',\n",
    "        # extra from kagglers\n",
    "        'colour': 'color',\n",
    "        'centre': 'center',\n",
    "        'favourite': 'favorite',\n",
    "        'travelling': 'traveling',\n",
    "        'counselling': 'counseling',\n",
    "        'theatre': 'theater',\n",
    "        'cancelled': 'canceled',\n",
    "        'labour': 'labor',\n",
    "        'organisation': 'organization',\n",
    "        'wwii': 'world war 2',\n",
    "        'citicise': 'criticize',\n",
    "        'youtu ': 'youtube ',\n",
    "        'sallary': 'salary',\n",
    "        'Whta': 'What',\n",
    "        'narcisist': 'narcissist',\n",
    "        'narcissit': 'narcissist',\n",
    "        'howdo': 'how do',\n",
    "        'whatare': 'what are',\n",
    "        'howcan': 'how can',\n",
    "        'howmuch': 'how much',\n",
    "        'howmany': 'how many',\n",
    "        'whydo': 'why do',\n",
    "        'doI': 'do I',\n",
    "        'theBest': 'the best',\n",
    "        'howdoes': 'how does',\n",
    "        'mastrubation': 'masturbation',\n",
    "        'mastrubate': 'masturbate',\n",
    "        'mastrubating': 'masturbating',\n",
    "        'pennis': 'penis',\n",
    "        'Etherium': 'Ethereum',\n",
    "        'bigdata': 'big data',\n",
    "        '2k17': '2017',\n",
    "        '2k18': '2018',\n",
    "        'qouta': 'quota',\n",
    "        'exboyfriend': 'ex boyfriend',\n",
    "        'airhostess': 'air hostess',\n",
    "        'whst': 'what',\n",
    "        'watsapp': 'whatsapp',\n",
    "        # extra\n",
    "        'bodyshame': 'body shaming',\n",
    "        'bodyshoppers': 'body shopping',\n",
    "        'bodycams': 'body cams',\n",
    "        'Cananybody': 'Can any body',\n",
    "        'deadbody': 'dead body',\n",
    "        'deaddict': 'de addict',\n",
    "        'Northindian': 'North Indian ',\n",
    "        'northindian': 'north Indian ',\n",
    "        'northkorea': 'North Korea',\n",
    "        'Whykorean': 'Why Korean',\n",
    "        'koreaboo': 'Korea boo ',\n",
    "        'Brexshit': 'British Exit bullshit',\n",
    "        'shithole': 'shithole ',\n",
    "        'shitpost': 'shit post',\n",
    "        'shitslam': 'shit Islam',\n",
    "        'shitlords': 'shit lords',\n",
    "        'Fck': 'Fuck',\n",
    "        'fck': 'fuck',\n",
    "        'Clickbait': 'click bait ',\n",
    "        'clickbait': 'click bait ',\n",
    "        'mailbait': 'mail bait',\n",
    "        'healhtcare': 'healthcare',\n",
    "        'trollbots': 'troll bots',\n",
    "        'trollled': 'trolled',\n",
    "        'trollimg': 'trolling',\n",
    "        'cybertrolling': 'cyber trolling',\n",
    "        'sickular': 'India sick secular ',\n",
    "        'suckimg': 'sucking',\n",
    "        'Idiotism': 'idiotism',\n",
    "        'Niggerism': 'Nigger',\n",
    "        'Niggeriah': 'Nigger'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def spacing_misspell(text):\n",
    "    \"\"\"\n",
    "    'deadbody' -> 'dead body'\n",
    "    \"\"\"\n",
    "    misspell_list = [\n",
    "        '(F|f)uck',\n",
    "        'Trump',\n",
    "        '\\W(A|a)nti',\n",
    "        '(W|w)hy',\n",
    "        '(W|w)hat',\n",
    "        'How',\n",
    "        'care\\W',\n",
    "        '\\Wover',\n",
    "        'gender',\n",
    "        'people',\n",
    "    ]\n",
    "    misspell_re = re.compile('(%s)' % '|'.join(misspell_list))\n",
    "    return misspell_re.sub(r\" \\1 \", text)\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', \n",
    "          '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "          '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  \n",
    "          '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”', \n",
    "          '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', \n",
    "          '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', \n",
    "          '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', \n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', \n",
    "          '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', \n",
    "          '¹', '≤', '‡', '√', 'β', 'α', '∅', 'θ', '÷', '₹']\n",
    "\n",
    "def space_punct(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: space_punct(x))\n",
    "print(\"Text cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7afaf857bd52305d1ced22575ab1193465eb4d2f"
   },
   "outputs": [],
   "source": [
    "def add_num_features(data):\n",
    "    data[\"total_length\"] = data[\"question_text\"].apply(len)\n",
    "    data[\"num_words\"] = data[\"question_text\"].str.count(\"\\S+\")\n",
    "    data[\"total_length\"] = data[\"total_length\"].fillna(0)\n",
    "    data[\"num_words\"] = data[\"num_words\"].fillna(0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scale_num_features(tr, val, te):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(tr)\n",
    "    \n",
    "    tr = scaler.transform(tr)\n",
    "    val = scaler.transform(val)\n",
    "    te = scaler.transform(te)\n",
    "    \n",
    "    return tr, val, te\n",
    "\n",
    "# train = add_num_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2b251f980d888277f765294454875fb0746b2004"
   },
   "outputs": [],
   "source": [
    "def get_glove(embedding_file):\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file))\n",
    "    \n",
    "#     all_embs = np.stack(embeddings_index.values())\n",
    "#     emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "#     embed_size = all_embs.shape[1]\n",
    "    return embeddings_index\n",
    "\n",
    "def get_para(embedding_file):\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file, \n",
    "                                                                   encoding=\"utf8\", \n",
    "                                                                   errors='ignore') if len(o)>100)\n",
    "#     all_embs = np.stack(embeddings_index.values())\n",
    "#     emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "#     embed_size = all_embs.shape[1]\n",
    "    return embeddings_index\n",
    "\n",
    "glove_index = get_glove(embedding_file1)\n",
    "# para_index = get_para(embedding_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "dc03cc0c03c1f6aa90a5883b5b0fd7f4543fecd4"
   },
   "outputs": [],
   "source": [
    "def get_embed(tokenizer = None, embeddings_index = None, emb_mean = None, emb_std = None):\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return nb_words, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "aa23238533b6116b494442aee5ed74f484408483",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punctuation = \"\".join(puncts)\n",
    "tokenizer = Tokenizer(num_words = max_features, \n",
    "                      filters = '\"#$%&()*+/:;-<=>@[\\]^_`{|}~', \n",
    "#                       filters = punctuation,\n",
    "                      lower = False)\n",
    "tokenizer.fit_on_texts(train[\"question_text\"])\n",
    "\n",
    "train_token = tokenizer.texts_to_sequences(train[\"question_text\"])\n",
    "train_seq = pad_sequences(train_token, maxlen = max_len)\n",
    "del train_token; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b04635fcfb3f298fe0f1bc8e9e2e9e1d0e3acc5d"
   },
   "outputs": [],
   "source": [
    "target = train[\"target\"].values\n",
    "# sincere = train[train[\"target\"] == 0][\"qid\"]\n",
    "# insincere = train[train[\"target\"] == 1][\"qid\"]\n",
    "\n",
    "# n_sincere = len(sincere)\n",
    "# n_insincere = len(insincere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b1e8c9d79a1fd9b2a44ae80d350a39cd4d0dada6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix completed!\n"
     ]
    }
   ],
   "source": [
    "nb_words, embedding_matrix1 = get_embed(tokenizer = tokenizer, embeddings_index = glove_index, \n",
    "                                        emb_mean = -0.005838499, \n",
    "                                        emb_std = 0.48782197)\n",
    "# nb_words, embedding_matrix2 = get_embed(tokenizer = tokenizer, embeddings_index = para_index, \n",
    "#                                         emb_mean = -0.0053247833, \n",
    "#                                         emb_std = 0.49346462)\n",
    "# embedding_matrix = np.mean([embedding_matrix1, embedding_matrix2], axis = 0)\n",
    "# del embedding_matrix1, embedding_matrix2; gc.collect()\n",
    "\n",
    "print(\"Embedding matrix completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a18006c730db43a5f475fa730c04fbe09ab40b7b"
   },
   "outputs": [],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import K\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "79c3edffe4c58f176a8d838f6c015c28f6489fa7"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "badeb5ed0293cdda9a16acc4bbe3ec81f9fab104"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d93a2827617386f8e1d11782a6610587a2d9e982"
   },
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    k-max-pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim = 3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "47b19cb4ef90135b615f585026fad85ffd5da066"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "import six\n",
    "import copy\n",
    "from six.moves import zip\n",
    "from keras.utils.generic_utils import serialize_keras_object\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Decoupled weight decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [Optimization for Deep Learning Highlights in 2017](http://ruder.io/deep-learning-optimization-2017/index.html)\n",
    "        - [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/6)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.init_lr = lr # decoupled weight decay (2/6)\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (3/6)\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd # decoupled weight decay (4/6)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "        eta_t = lr / self.init_lr # decoupled weight decay (5/6)\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - eta_t * wd * p # decoupled weight decay (6/6)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f20d3be44e90cfef507461416d49dfc89d1a9f2a"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Wrapper\n",
    "import keras.backend as K\n",
    "\n",
    "class DropConnect(Wrapper):\n",
    "    def __init__(self, layer, prob=1., **kwargs):\n",
    "        self.prob = prob\n",
    "        self.layer = layer\n",
    "        super(DropConnect, self).__init__(layer, **kwargs)\n",
    "        if 0. < self.prob < 1.:\n",
    "            self.uses_learning_phase = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(DropConnect, self).build()\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        if 0. < self.prob < 1.:\n",
    "            self.layer.kernel = K.in_train_phase(K.dropout(self.layer.kernel, self.prob), self.layer.kernel)\n",
    "            self.layer.bias = K.in_train_phase(K.dropout(self.layer.bias, self.prob), self.layer.bias)\n",
    "        return self.layer.call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "fac77ba8814fc6a5e2b6eeac9b3050ce4d3466ff"
   },
   "outputs": [],
   "source": [
    "def get_f1(true, val):\n",
    "    precision, recall, thresholds = precision_recall_curve(true, val)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_threshold = thresholds[np.argmax(F)]\n",
    "    \n",
    "    return best_threshold, best_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "f96bd834baf49114483c368d1d13f10d20bc1103"
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(units = 40, dr = 0.3, \n",
    "                     num_capsules = 10, dim_capsules = 10, \n",
    "                     routs = 4, _k = 2,\n",
    "                     nb_words = nb_words, embedding_matrix = embedding_matrix1):\n",
    "    \n",
    "    inp = Input(shape = (max_len, ))\n",
    "#     num_inp = Input(shape = (num_feat.shape[1], ))\n",
    "    embed_layer = Embedding(nb_words, embed_size, input_length = max_len,\n",
    "                            weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x = SpatialDropout1D(dr, seed = seed)(embed_layer)\n",
    "    \n",
    "#     x = GaussianDropout(dr)(embed_layer)\n",
    "    x = Bidirectional(CuDNNLSTM(units, kernel_initializer = glorot_normal(seed = seed), \n",
    "                                recurrent_initializer = orthogonal(gain = 1.0, seed = seed), \n",
    "                                return_sequences = True))(x)\n",
    "    x = Capsule(num_capsule = num_capsules, dim_capsule = dim_capsules, routings = routs)(x)\n",
    "    x, x_h, x_c = Bidirectional(CuDNNGRU(units, kernel_initializer = glorot_normal(seed = seed),\n",
    "                                         recurrent_initializer = orthogonal(gain = 1.0, seed = seed),\n",
    "                                         return_sequences = True, return_state = True))(x)\n",
    "#     x = Conv1D(32, 2, strides = 2, padding = \"valid\", \n",
    "#                kernel_initializer = glorot_normal(seed = seed))(x)\n",
    "#    att = Attention(num_capsule//2)(x) # if conv1d applied\n",
    "\n",
    "    att = Attention(num_capsules)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "#     k_max_pool = KMaxPooling(k = _k)(x)\n",
    "    \n",
    "    main = concatenate([x_h, att, avg_pool, max_pool])\n",
    "#     main = Dense(256, kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "#     main = Activation(\"relu\")(main)\n",
    "#     main = Dropout(0.2, seed = seed)(main)\n",
    "    main = DropConnect(Dense(256, kernel_initializer = glorot_normal(seed = seed)), 0.1)(main)\n",
    "#     main = Dense(64, kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "    main = Activation(\"relu\")(main)\n",
    "#     main = Dropout(0.1, seed = seed)(main)\n",
    "    \n",
    "#     main = Dense(32, kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "#     main = Activation(\"relu\")(main)\n",
    "#     main = Dropout(0.05, seed = seed)(main)\n",
    "    \n",
    "    out = Dense(1, activation = \"sigmoid\", \n",
    "                kernel_initializer = glorot_normal(seed = seed))(main)\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    model.compile(loss = \"binary_crossentropy\",\n",
    "                  optimizer = AdamW(weight_decay = 0.5e-4), \n",
    "                  metrics = None)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "345237815a7e7fbbcb22123283c1de02d5e216b4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Training 1/5 fold\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/9\n",
      " - 136s - loss: 0.1311 - val_loss: 0.1114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11142, saving model to fold_1_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1093 - val_loss: 0.1036\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11142 to 0.10363, saving model to fold_1_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1040 - val_loss: 0.1019\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10363 to 0.10191, saving model to fold_1_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1009 - val_loss: 0.0991\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10191 to 0.09906, saving model to fold_1_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0979 - val_loss: 0.0982\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09906 to 0.09815, saving model to fold_1_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0949 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09815 to 0.09806, saving model to fold_1_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0920 - val_loss: 0.0960\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09806 to 0.09600, saving model to fold_1_best_model.h5\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0896 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09600 to 0.09533, saving model to fold_1_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0882 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.3771718442440033 is 0.6931580172213172.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 2/5 fold\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/9\n",
      " - 133s - loss: 0.1313 - val_loss: 0.1093\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10927, saving model to fold_2_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1090 - val_loss: 0.1036\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10927 to 0.10357, saving model to fold_2_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1042 - val_loss: 0.1010\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10357 to 0.10095, saving model to fold_2_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1007 - val_loss: 0.0996\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10095 to 0.09965, saving model to fold_2_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0976 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09965 to 0.09893, saving model to fold_2_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0946 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09893 to 0.09782, saving model to fold_2_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0919 - val_loss: 0.0965\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09782 to 0.09648, saving model to fold_2_best_model.h5\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0897 - val_loss: 0.0963\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09648 to 0.09631, saving model to fold_2_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0882 - val_loss: 0.0979\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09631\n",
      "F1 score at threshold 0.3656523823738098 is 0.6898087353696832.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 3/5 fold\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/9\n",
      " - 133s - loss: 0.1313 - val_loss: 0.1058\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10583, saving model to fold_3_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1089 - val_loss: 0.1019\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10583 to 0.10188, saving model to fold_3_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1042 - val_loss: 0.1004\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10188 to 0.10040, saving model to fold_3_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1009 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10040 to 0.09784, saving model to fold_3_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0981 - val_loss: 0.0968\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09784 to 0.09684, saving model to fold_3_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0952 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09684 to 0.09470, saving model to fold_3_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0924 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09470 to 0.09436, saving model to fold_3_best_model.h5\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0901 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09436 to 0.09400, saving model to fold_3_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0885 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09400 to 0.09396, saving model to fold_3_best_model.h5\n",
      "F1 score at threshold 0.35094016790390015 is 0.6953294871423188.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 4/5 fold\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/9\n",
      " - 133s - loss: 0.1312 - val_loss: 0.1129\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11295, saving model to fold_4_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1091 - val_loss: 0.1026\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11295 to 0.10258, saving model to fold_4_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1041 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10258 to 0.10046, saving model to fold_4_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1008 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10046 to 0.09916, saving model to fold_4_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0981 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09916 to 0.09768, saving model to fold_4_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0946 - val_loss: 0.0969\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09768 to 0.09693, saving model to fold_4_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0920 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09693 to 0.09605, saving model to fold_4_best_model.h5\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0896 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09605 to 0.09535, saving model to fold_4_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0883 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09535\n",
      "F1 score at threshold 0.36642345786094666 is 0.6901060584047654.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training 5/5 fold\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/9\n",
      " - 133s - loss: 0.1310 - val_loss: 0.1090\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10896, saving model to fold_5_best_model.h5\n",
      "Epoch 2/9\n",
      " - 131s - loss: 0.1087 - val_loss: 0.1040\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10896 to 0.10405, saving model to fold_5_best_model.h5\n",
      "Epoch 3/9\n",
      " - 131s - loss: 0.1036 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10405 to 0.10204, saving model to fold_5_best_model.h5\n",
      "Epoch 4/9\n",
      " - 131s - loss: 0.1005 - val_loss: 0.0994\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10204 to 0.09937, saving model to fold_5_best_model.h5\n",
      "Epoch 5/9\n",
      " - 131s - loss: 0.0977 - val_loss: 0.0984\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09937 to 0.09844, saving model to fold_5_best_model.h5\n",
      "Epoch 6/9\n",
      " - 131s - loss: 0.0945 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09844 to 0.09645, saving model to fold_5_best_model.h5\n",
      "Epoch 7/9\n",
      " - 131s - loss: 0.0920 - val_loss: 0.0966\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09645\n",
      "Epoch 8/9\n",
      " - 131s - loss: 0.0894 - val_loss: 0.0951\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09645 to 0.09513, saving model to fold_5_best_model.h5\n",
      "Epoch 9/9\n",
      " - 131s - loss: 0.0879 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09513\n",
      "F1 score at threshold 0.39201632142066956 is 0.6932887677016447.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold = 5\n",
    "batch_size = 1024\n",
    "epochs = 9\n",
    "units, dr = 64, 0.3\n",
    "num_capsules = 10\n",
    "dim_capsules = 10\n",
    "routs = 3\n",
    "k = 3\n",
    "\n",
    "oof_pred = np.zeros((len(target), 1))\n",
    "thresholds = [0]*fold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = fold, shuffle = True, random_state = seed)\n",
    "fold_idx = kfold.split(train_seq, target)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(fold_idx):\n",
    "    f = i+1\n",
    "    print(\"-\"*70)\n",
    "    print(\"Training {}/{} fold\".format(i+1, fold))\n",
    "    \n",
    "#     train_idx = pd.concat([sincere[:int(n_sincere*(i/fold))], \n",
    "#                            sincere[int(n_sincere*(f/fold)):],\n",
    "#                            insincere[:int(n_insincere*(i/fold))], \n",
    "#                            insincere[int(n_insincere*(f/fold)):]]).index\n",
    "#     val_idx = pd.concat([sincere[int(n_sincere*(i/fold)):int(n_sincere*(f/fold))],\n",
    "#                          insincere[int(n_insincere*(i/fold)):int(n_insincere*(f/fold))]]).index\n",
    "    \n",
    "    X_train, y_train = train_seq[train_idx], target[train_idx]\n",
    "    X_val, y_val = train_seq[val_idx], target[val_idx]\n",
    "    \n",
    "#     test_num = test[[\"total_length\", \"num_words\"]]\n",
    "#     train_num, val_num, test_num = scale_num_features(train_num, val_num, test_num)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    train_idx = np.random.permutation(len(X_train))\n",
    "#     val_idx = np.random.permutation(len(X_val))\n",
    "\n",
    "    X_train = X_train[train_idx]\n",
    "#     X_val = X_val[val_idx]\n",
    "#     train_num = train_num[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "#     y_val = y_val[val_idx]\n",
    "\n",
    "    best_model = \"fold_{}_best_model.h5\".format(f)\n",
    "    check_point = ModelCheckpoint(best_model, monitor = \"val_loss\", mode = \"min\",\n",
    "                                  save_best_only = True, \n",
    "                                  save_weights_only = True,\n",
    "                                  verbose = 1)\n",
    "    clr = CyclicLR(base_lr = 0.0008, max_lr = 0.0025,\n",
    "                   step_size = 4*int(len(X_train)/batch_size), \n",
    "                   mode = \"exp_range\",\n",
    "                   gamma = 0.99994)\n",
    "\n",
    "    K.clear_session()\n",
    "    model = build_lstm_model(units = units, dr = dr, \n",
    "                             num_capsules = num_capsules, dim_capsules = dim_capsules, \n",
    "                             routs = routs, _k = k,\n",
    "                             nb_words = nb_words, embedding_matrix = embedding_matrix1)\n",
    "    \n",
    "    lstm_hist = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n",
    "                          validation_data = (X_val, y_val), \n",
    "                          callbacks = [check_point, clr],\n",
    "                          verbose = 2)\n",
    "\n",
    "    model.load_weights(best_model)\n",
    "    val_pred = model.predict(X_val, batch_size = batch_size, verbose = 2)\n",
    "    threshold, score = get_f1(y_val, val_pred)\n",
    "    oof_pred[val_idx] = val_pred\n",
    "    thresholds[i] = threshold\n",
    "    \n",
    "    print(\"F1 score at threshold {} is {}.\\n\".format(threshold, score))\n",
    "    del model; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "ed0aa5f3a212e37591ce47b4c9feb2e8889c7ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds of each fold: \n",
      "[0.3771718442440033, 0.3656523823738098, 0.35094016790390015, 0.36642345786094666, 0.39201632142066956]\n",
      "F1 score at threshold 0.3659 is 0.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"Thresholds of each fold: \\n{}\".format(thresholds))\n",
    "threshold, score = get_f1(target, oof_pred)\n",
    "print(\"F1 score at threshold {:.4f} is {:.4f}\".format(threshold, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "68f7c2425d8eabe21a0ed7dea7a78efd1685af60"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/test.csv\").fillna(\"missing\")\n",
    "\n",
    "num_test = test.shape[0]\n",
    "pred = np.zeros((num_test, 1))\n",
    "\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: space_punct(x))\n",
    "\n",
    "test_token = tokenizer.texts_to_sequences(test[\"question_text\"])\n",
    "X_test = pad_sequences(test_token, maxlen = max_len)\n",
    "del test_token; gc.collect()\n",
    "\n",
    "for f in range(1, 6):\n",
    "    model = build_lstm_model(units = units, dr = dr, \n",
    "                             num_capsules = num_capsules, dim_capsules = dim_capsules, \n",
    "                             routs = routs, _k = k,\n",
    "                             nb_words = nb_words, embedding_matrix = embedding_matrix1)\n",
    "    model.load_weights(\"fold_{}_best_model.h5\".format(f))\n",
    "    pred += model.predict(X_test, batch_size = batch_size, verbose = 2)\n",
    "    \n",
    "pred = pred/fold\n",
    "pred = (pred > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "939dc70d92d308ccad11e2c5fb36e1626955540b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00014894849d00ba98a9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000156468431f09b3cae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000227734433360e1aae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005e06fbe3045bd2a92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00068a0f7f41f50fc399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  prediction\n",
       "0  00014894849d00ba98a9           0\n",
       "1  000156468431f09b3cae           0\n",
       "2  000227734433360e1aae           0\n",
       "3  0005e06fbe3045bd2a92           0\n",
       "4  00068a0f7f41f50fc399           0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"prediction\"] = pred\n",
    "test[[\"qid\", \"prediction\"]].to_csv(\"submission.csv\", index = False)\n",
    "test[[\"qid\", \"prediction\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5810056862850c8c84771949edc9f7926cc28f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Predict completed in 6331.5123999118805\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Train and Predict completed in {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "a5e28b3f1fcb67fb423d986cdf4221a5a365f38f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "40b33e92e0f3c1f8cc7359a7e8fbe6820d5fbc7f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
