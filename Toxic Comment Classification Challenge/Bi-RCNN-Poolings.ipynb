{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "af8cKGjF6qu_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "2bdd9da9-02d0-4c4e-9ea4-1f53ade2284e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520204272576,
          "user_tz": 480,
          "elapsed": 2565,
          "user": {
            "displayName": "Rong Hui Yu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117367515583238213291"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "  Downloading Keras-2.1.4-py2.py3-none-any.whl (322kB)\n",
            "\u001b[K    100% |████████████████████████████████| 327kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qczp3NiU6XC0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# FastTest\n",
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
        "!unzip crawl-300d-2M.vec.zip -d crawl-300d-2M.vec/\n",
        "\n",
        "# Twitter\n",
        "# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip -c\n",
        "# !unzip glove.twitter.27B.zip -d glove.twitter.27B.200d/\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(32)\n",
        "import pandas as pd\n",
        "\n",
        "! pip install pydrive\n",
        "# these classes allow you to request the Google drive API\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# https://drive.google.com/open?id=1Rk9F9YUMuG9JtuhCaUck1Lxid3OISc1-\n",
        "file_id = '1Rk9F9YUMuG9JtuhCaUck1Lxid3OISc1-'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "# allows you to temporarily load your file in the notebook VM\n",
        "\n",
        "downloaded.GetContentFile('train.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "# https://drive.google.com/open?id=1CH6MLJYHK6rtC-p_4kK7Ms17CIisowGZ\n",
        "file_id = '1CH6MLJYHK6rtC-p_4kK7Ms17CIisowGZ'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('test.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFjg7drSBIvC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "embed_size = 300\n",
        "max_features = 100000 \n",
        "max_text_len = 150\n",
        "\n",
        "# EMBEDDING_FILE = \"glove.840B.300d/glove.840B.300d.txt\"\n",
        "# EMBEDDING_FILE = \"glove.twitter.27B.200d/glove.twitter.27B.200d.txt\"\n",
        "EMBEDDING_FILE = \"crawl-300d-2M.vec/crawl-300d-2M.vec\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KLnHJwFJ6ruo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ecb5825-4038-44fb-cc1a-a996cafd06c7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520204391911,
          "user_tz": 480,
          "elapsed": 7814,
          "user": {
            "displayName": "Rong Hui Yu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117367515583238213291"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import sys, os, re, csv, codecs, gc\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
        "from keras.layers import Bidirectional, Add, Flatten\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "\n",
        "import logging\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch + 1, score))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "65Xm9gZc6t8M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    text = text.replace('&', ' and ')\n",
        "    text = text.replace('@', ' at ')\n",
        "    text = text.replace('0', ' zero ')\n",
        "    text = text.replace('1', ' one ')\n",
        "    text = text.replace('2', ' two ')\n",
        "    text = text.replace('3', ' three ')\n",
        "    text = text.replace('4', ' four ')\n",
        "    text = text.replace('5', ' five ')\n",
        "    text = text.replace('6', ' six ')\n",
        "    text = text.replace('7', ' seven ')\n",
        "    text = text.replace('8', ' eight ')\n",
        "    text = text.replace('9', ' nine ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUmmk494cwTj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5210ca79-45a6-4b16-e8c6-cdcba1240aca",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520204615051,
          "user_tz": 480,
          "elapsed": 1252,
          "user": {
            "displayName": "Rong Hui Yu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117367515583238213291"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_corpus(comment):\n",
        "    comment = comment.lower()\n",
        "    text = nltk.word_tokenize(comment)\n",
        "    new_text = \" \".join(word for word in text)\n",
        "    return new_text"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n58MVlV86yUB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "category = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "train[\"comment_text\"].fillna(\"no comment\", inplace = True)\n",
        "train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: normalize(x))\n",
        "train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: clean_corpus(x))\n",
        "\n",
        "test[\"comment_text\"].fillna(\"no comment\", inplace = True)\n",
        "test[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: normalize(x))\n",
        "test[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: clean_corpus(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bqzD1V-86zTg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f70dae15-b13c-4ef5-c99e-4908d72663b2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520204835483,
          "user_tz": 480,
          "elapsed": 29794,
          "user": {
            "displayName": "Rong Hui Yu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117367515583238213291"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "nrow_train = train.shape[0]\n",
        "Y_train = train[category].values\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(train, Y_train, test_size = 0.1)\n",
        "\n",
        "raw_text_train = X_train[\"comment_text\"].str.lower()\n",
        "raw_text_valid = X_valid[\"comment_text\"].str.lower()\n",
        "raw_text_test = test[\"comment_text\"].str.lower()\n",
        "\n",
        "tk = Tokenizer(num_words = max_features, lower = True)\n",
        "tk.fit_on_texts(raw_text_train)\n",
        "X_train[\"comment_seq\"] = tk.texts_to_sequences(raw_text_train.str.lower())\n",
        "X_valid[\"comment_seq\"] = tk.texts_to_sequences(raw_text_valid.str.lower())\n",
        "test[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test.str.lower())\n",
        "\n",
        "X_train = pad_sequences(X_train.comment_seq, maxlen = max_text_len)\n",
        "X_valid = pad_sequences(X_valid.comment_seq, maxlen = max_text_len)\n",
        "X_test = pad_sequences(test.comment_seq, maxlen = max_text_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QxUCe0U860L1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype = \"float32\")\n",
        "embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIhjgIiw61mr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9vYxTO59Gij",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AttentionWeightedAverage(Layer):\n",
        "    \"\"\"\n",
        "    Computes a weighted average of the different channels across timesteps.\n",
        "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, return_attention=False, **kwargs):\n",
        "        self.init = initializers.get('uniform')\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(ndim=3)]\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 initializer=self.init)\n",
        "        self.trainable_weights = [self.W]\n",
        "        super(AttentionWeightedAverage, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # computes a probability distribution over the timesteps\n",
        "        # uses 'max trick' for numerical stability\n",
        "        # reshape is done to avoid issue with Tensorflow\n",
        "        # and 1-dimensional weights\n",
        "        logits = K.dot(x, self.W)\n",
        "        x_shape = K.shape(x)\n",
        "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
        "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
        "\n",
        "        # masked timesteps have zero weight\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            ai = ai * mask\n",
        "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
        "        weighted_input = x * K.expand_dims(att_weights)\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "        if self.return_attention:\n",
        "            return [result, att_weights]\n",
        "        return result\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return self.compute_output_shape(input_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_len = input_shape[2]\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
        "        return (input_shape[0], output_len)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return [None] * len(input_mask)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "def pair_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "    parts = tf.dynamic_partition(y_pred, y_true, 2)\n",
        "    y_pos = parts[1]\n",
        "    y_neg = parts[0]\n",
        "    y_pos = tf.expand_dims(y_pos, 0)\n",
        "    y_neg = tf.expand_dims(y_neg, -1)\n",
        "    out = K.sigmoid(y_neg - y_pos)\n",
        "    return K.mean(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2cJSXOS62uO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers import SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import concatenate, GRU, CuDNNGRU, CuDNNLSTM, MaxPooling1D, TimeDistributed\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def build_model(units = 0, dr = 0.0, lr_i = 0.0, lr_f = 0.0):\n",
        "  inp = Input(shape = (max_text_len,))\n",
        "  main = Embedding(nb_words, embed_size, weights = [embedding_matrix], \n",
        "                   input_length = max_text_len, trainable = False)(inp)\n",
        "  \n",
        "  main = SpatialDropout1D(dr)(main)\n",
        "  main = Bidirectional(CuDNNLSTM(units, return_sequences = True))(main)\n",
        "  main = Conv1D(64, kernel_size = 3, padding = \"same\", kernel_initializer = \"he_uniform\", activation = \"elu\")(main)\n",
        "  atten = AttentionWeightedAverage()(main)\n",
        "  max_pool = GlobalMaxPooling1D()(main)\n",
        "  main = concatenate([atten, max_pool])\n",
        "  \n",
        "  out_put = Dense(6, activation = \"sigmoid\")(main)\n",
        "  model = Model(inputs = inp, outputs = out_put)\n",
        "  model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr_i, decay = lr_f), metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lXK9dwd65Bp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 41
            },
            {
              "item_id": 193
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1045
        },
        "outputId": "4aea0263-b686-4391-81bc-b58eb97431fc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520221393104,
          "user_tz": 480,
          "elapsed": 1100472,
          "user": {
            "displayName": "Rong Hui Yu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117367515583238213291"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
        "\n",
        "batch_sizes = 128\n",
        "epochs = 30\n",
        "units = 128\n",
        "dr = 0.3\n",
        "lr_i, lr_f = 1e-3, 0\n",
        "\n",
        "file_path = \"best_model.hdf5\"\n",
        "model = build_model(units = units, dr = dr, lr_i = lr_i)\n",
        "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                              save_best_only = True, mode = \"min\", save_weights_only = True)\n",
        "ra_val = RocAucEvaluation(validation_data = (X_valid, Y_valid), interval = 1)\n",
        "history = model.fit(X_train, Y_train, batch_size = batch_sizes, epochs = epochs, validation_data = (X_valid, Y_valid), \n",
        "                    verbose = 2, callbacks = [check_point, early_stop, ra_val])\n",
        "model.load_weights(file_path)\n",
        "pred = model.predict(X_test, batch_size = batch_sizes, verbose = 1)\n",
        "\n",
        "# n_folds = 5\n",
        "# cv_scores = []\n",
        "# pred = 0\n",
        "\n",
        "# kfold = KFold(n_splits = n_folds, shuffle = True, random_state = 32)\n",
        "# for i, (train_idx, valid_idx) in enumerate(kfold.split(X_train)):\n",
        "#   print(\"\\nRunning fold {}/{}\".format(i + 1, n_folds))\n",
        "#   model = None\n",
        "#   model = build_LSTM_model(units = units, dr = dr, lr_i = lr_i, lr_f = lr_f)\n",
        "  \n",
        "#   x_train, y_train = X_train[train_idx], Y_train[train_idx] \n",
        "#   x_valid, y_valid = X_train[valid_idx], Y_train[valid_idx]\n",
        "  \n",
        "#   ra_val = RocAucEvaluation(validation_data = (x_valid, y_valid), interval = 1)\n",
        "\n",
        "#   file_path = \"fold \" + str(i+1) + \" best_model.hdf5\"\n",
        "#   check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "#                                 save_best_only = True, mode = \"min\", save_weights_only = True)\n",
        "  \n",
        "#   history = model.fit(x_train, y_train, batch_size = batch_sizes, epochs = epochs, validation_data = (x_valid, y_valid), \n",
        "#                       verbose = 2, callbacks = [check_point, early_stop, ra_val])\n",
        "#   model.load_weights(file_path)\n",
        "#   pred += model.predict(X_test, batch_size = batch_sizes, verbose = 1)\n",
        "  \n",
        "#   oof_pred = model.predict(x_valid, batch_size = batch_sizes, verbose = 1)\n",
        "#   cv_score = roc_auc_score(y_valid, oof_pred)\n",
        "#   cv_scores.append(cv_score)\n",
        "  \n",
        "#   del file_path, model\n",
        "#   gc.collect()\n",
        "  \n",
        "# pred = pred/n_flods\n",
        "# print(\"\\nAverage cv score of {} folds is {}.\".format(n_folds, np.mean(cv_scores)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/30\n",
            " - 110s - loss: 0.0612 - acc: 0.9787 - val_loss: 0.0427 - val_acc: 0.9836\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04268, saving model to best_model.hdf5\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.987325\n",
            "Epoch 2/30\n",
            " - 108s - loss: 0.0442 - acc: 0.9831 - val_loss: 0.0395 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04268 to 0.03946, saving model to best_model.hdf5\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.990321\n",
            "Epoch 3/30\n",
            " - 108s - loss: 0.0417 - acc: 0.9839 - val_loss: 0.0389 - val_acc: 0.9848\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03946 to 0.03887, saving model to best_model.hdf5\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.991020\n",
            "Epoch 4/30\n",
            " - 108s - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0382 - val_acc: 0.9848\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03887 to 0.03823, saving model to best_model.hdf5\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.991418\n",
            "Epoch 5/30\n",
            " - 108s - loss: 0.0379 - acc: 0.9854 - val_loss: 0.0392 - val_acc: 0.9848\n",
            "\n",
            "Epoch 00005: val_loss did not improve\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.991241\n",
            "Epoch 6/30\n",
            " - 107s - loss: 0.0363 - acc: 0.9859 - val_loss: 0.0396 - val_acc: 0.9846\n",
            "\n",
            "Epoch 00006: val_loss did not improve\n",
            "\n",
            " ROC-AUC - epoch: 6 - score: 0.991216\n",
            "Epoch 7/30\n",
            " - 108s - loss: 0.0345 - acc: 0.9864 - val_loss: 0.0395 - val_acc: 0.9840\n",
            "\n",
            "Epoch 00007: val_loss did not improve\n",
            "\n",
            " ROC-AUC - epoch: 7 - score: 0.991177\n",
            "Epoch 8/30\n",
            " - 108s - loss: 0.0326 - acc: 0.9872 - val_loss: 0.0400 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00008: val_loss did not improve\n",
            "\n",
            " ROC-AUC - epoch: 8 - score: 0.991235\n",
            "Epoch 9/30\n",
            " - 108s - loss: 0.0307 - acc: 0.9879 - val_loss: 0.0405 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00009: val_loss did not improve\n",
            "\n",
            " ROC-AUC - epoch: 9 - score: 0.990634\n",
            " 10368/153164 [=>............................] - ETA: 38s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "153164/153164 [==============================] - 41s 268us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0dQh_iyC6-8F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "submission = submission.reindex(columns = [\"id\"] + category)\n",
        "submission[\"id\"]= test[[\"id\"]]\n",
        "submission[category[0:6]] = lstm_pred\n",
        "submission.to_csv(\"submission.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pp91FzfL7AFY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b0afc5ea-aca0-44d2-b29f-3818269abba1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520221397404,
          "user_tz": 480,
          "elapsed": 285,
          "user": {
            "displayName": "Rong Hui Yu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117367515583238213291"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "submission.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>0.998902</td>\n",
              "      <td>0.376713</td>\n",
              "      <td>0.982758</td>\n",
              "      <td>0.130508</td>\n",
              "      <td>0.936714</td>\n",
              "      <td>0.549623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>0.001845</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>0.006562</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>0.000224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
              "0  00001cee341fdb12  0.998902      0.376713  0.982758  0.130508  0.936714   \n",
              "1  0000247867823ef7  0.000755      0.000037  0.000466  0.000040  0.000245   \n",
              "2  00013b17ad220c46  0.001845      0.000129  0.001026  0.000109  0.000440   \n",
              "3  00017563c3f7919a  0.000440      0.000028  0.000292  0.000042  0.000144   \n",
              "4  00017695ad8997eb  0.006562      0.000128  0.000927  0.000261  0.000601   \n",
              "\n",
              "   identity_hate  \n",
              "0       0.549623  \n",
              "1       0.000090  \n",
              "2       0.000446  \n",
              "3       0.000017  \n",
              "4       0.000224  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "K0P_57geniNA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
