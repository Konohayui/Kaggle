{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hierarchical Attention Network.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"h8RCgumDPvCj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["! pip install keras"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6dWTSkc2P1Gw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Glove\n","# !wget http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip -c\n","# !unzip glove.840B.300d.zip -d glove.840B.300d/\n","\n","# Twitter\n","# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip -c\n","# !unzip glove.twitter.27B.zip -d glove.twitter.27B.200d/\n","\n","# FastTest\n","!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n","!unzip crawl-300d-2M.vec.zip -d crawl-300d-2M.vec/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DIW91yd8P1te","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","np.random.seed(32)\n","import pandas as pd\n","\n","! pip install pydrive\n","# these classes allow you to request the Google drive API\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive \n","from google.colab import auth \n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","# https://drive.google.com/open?id=1Rk9F9YUMuG9JtuhCaUck1Lxid3OISc1-\n","file_id = '1Rk9F9YUMuG9JtuhCaUck1Lxid3OISc1-'\n","downloaded = drive.CreateFile({'id': file_id})\n","# allows you to temporarily load your file in the notebook VM\n","\n","downloaded.GetContentFile('train.csv')\n","train = pd.read_csv('train.csv')\n","\n","# https://drive.google.com/open?id=1CH6MLJYHK6rtC-p_4kK7Ms17CIisowGZ\n","file_id = '1CH6MLJYHK6rtC-p_4kK7Ms17CIisowGZ'\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('test.csv')\n","test = pd.read_csv('test.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NBEm_Kb7P4Hk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["embed_size = 300\n","max_features = 150000 \n","max_text_len = 150\n","max_sent = 10\n","\n","# EMBEDDING_FILE = \"glove.840B.300d/glove.840B.300d.txt\"\n","# EMBEDDING_FILE = \"glove.twitter.27B.200d/glove.twitter.27B.200d.txt\"\n","EMBEDDING_FILE = \"crawl-300d-2M.vec/crawl-300d-2M.vec\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vKlLbNq2P6Bz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sys, os, re, csv, codecs, gc\n","os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n","from sklearn.model_selection import train_test_split\n","import nltk\n","from nltk import tokenize, word_tokenize\n","nltk.download(\"punkt\")\n","\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, GRU, LSTM, Embedding, Dropout, Activation, Conv1D\n","from keras.layers import Bidirectional, Add, Flatten, CuDNNGRU, CuDNNLSTM, TimeDistributed\n","from keras.optimizers import Adam, RMSprop, SGD, Nadam\n","\n","from keras.models import Model, load_model\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import backend as K\n","from keras.engine import InputSpec, Layer\n","\n","import logging\n","from sklearn.metrics import roc_auc_score\n","from keras.callbacks import Callback\n","\n","class RocAucEvaluation(Callback):\n","    def __init__(self, validation_data=(), interval=1):\n","        super(Callback, self).__init__()\n","\n","        self.interval = interval\n","        self.X_val, self.y_val = validation_data\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch % self.interval == 0:\n","            y_pred = self.model.predict(self.X_val, verbose=0)\n","            score = roc_auc_score(self.y_val, y_pred)\n","            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch + 1, score))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D6gTcH5DP-Dz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class AttentionWeightedAverage(Layer):\n","    \"\"\"\n","    Computes a weighted average of the different channels across timesteps.\n","    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n","    \"\"\"\n","\n","    def __init__(self, return_attention=False, **kwargs):\n","        self.init = initializers.get('uniform')\n","        self.supports_masking = True\n","        self.return_attention = return_attention\n","        super(AttentionWeightedAverage, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.input_spec = [InputSpec(ndim=3)]\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight(shape=(input_shape[2], 1),\n","                                 name='{}_W'.format(self.name),\n","                                 initializer=self.init)\n","        self.trainable_weights = [self.W]\n","        super(AttentionWeightedAverage, self).build(input_shape)\n","\n","    def call(self, x, mask=None):\n","        # computes a probability distribution over the timesteps\n","        # uses 'max trick' for numerical stability\n","        # reshape is done to avoid issue with Tensorflow\n","        # and 1-dimensional weights\n","        logits = K.dot(x, self.W)\n","        x_shape = K.shape(x)\n","        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n","        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n","\n","        # masked timesteps have zero weight\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            ai = ai * mask\n","        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n","        weighted_input = x * K.expand_dims(att_weights)\n","        result = K.sum(weighted_input, axis=1)\n","        if self.return_attention:\n","            return [result, att_weights]\n","        return result\n","\n","    def get_output_shape_for(self, input_shape):\n","        return self.compute_output_shape(input_shape)\n","\n","    def compute_output_shape(self, input_shape):\n","        output_len = input_shape[2]\n","        if self.return_attention:\n","            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n","        return (input_shape[0], output_len)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        if isinstance(input_mask, list):\n","            return [None] * len(input_mask)\n","        else:\n","            return None\n","\n","\n","def pair_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, tf.int32)\n","    parts = tf.dynamic_partition(y_pred, y_true, 2)\n","    y_pos = parts[1]\n","    y_neg = parts[0]\n","    y_pos = tf.expand_dims(y_pos, 0)\n","    y_neg = tf.expand_dims(y_neg, -1)\n","    out = K.sigmoid(y_neg - y_pos)\n","    return K.mean(out)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bh2T45uDP--e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def clean_corpus(comment):\n","    comment = comment.lower()\n","    comment = comment.replace('&', ' and ')\n","    comment = comment.replace('0', ' zero ')\n","    comment = comment.replace('1', ' one ')\n","    comment = comment.replace('2', ' two ')\n","    comment = comment.replace('3', ' three ')\n","    comment = comment.replace('4', ' four ')\n","    comment = comment.replace('5', ' five ')\n","    comment = comment.replace('6', ' six ')\n","    comment = comment.replace('7', ' seven ')\n","    comment = comment.replace('8', ' eight ')\n","    comment = comment.replace('9', ' nine ')\n","    comment = comment.replace('\\'ve', ' have ')\n","    comment = comment.replace('\\'d', ' would ')\n","    comment = comment.replace('\\'m', ' am ')\n","    comment = comment.replace('n\\'t', ' not ')\n","    comment = comment.replace('\\'s', ' is ')\n","    comment = comment.replace('\\'r', ' are ')\n","    comment = re.sub(r'\\\\', '', comment)\n","    comment = nltk.word_tokenize(comment)\n","    comment = \" \".join(word for word in comment)\n","    return comment"],"execution_count":0,"outputs":[]},{"metadata":{"id":"krlkCkLHQHKS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["category = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","train[\"comment_text\"].fillna(\"no comment\", inplace = True)\n","train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: clean_corpus(x))\n","\n","test[\"comment_text\"].fillna(\"no comment\", inplace = True)\n","test[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: clean_corpus(x))\n","\n","train[\"sentences\"] = train[\"comment_text\"].apply(lambda x: tokenize.sent_tokenize(x))\n","test[\"sentences\"] = test[\"comment_text\"].apply(lambda x: tokenize.sent_tokenize(x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bTbYWawxQKNA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["Y_train = train[category].values\n","X_train, X_valid, Y_train, Y_valid = train_test_split(train, Y_train, test_size = 0.1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T00ty8sTQeiY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","\n","raw_text = X_train[\"comment_text\"]\n","tk = Tokenizer(num_words = max_features, lower = True)\n","tk.fit_on_texts(raw_text)\n","\n","def sentenize(data):\n","    comments = data[\"sentences\"]\n","    sent_matrix = np.zeros((comments.shape[0], max_sent, max_text_len), dtype = \"int32\")\n","    for i, sentences in enumerate(comments):\n","        for j, sent in enumerate(sentences):\n","            if j < max_sent:\n","                wordTokens = text_to_word_sequence(sent)\n","                k=0\n","                for _, word in enumerate(wordTokens):\n","                    try:\n","                        if k < max_text_len and tk.word_index[word] < max_features:\n","                            sent_matrix[i, j, k] = tk.word_index[word]\n","                            k = k+1 \n","                    except:\n","                            sent_matrix[i, j, k] = 0\n","                            k = k+1\n","    return sent_matrix\n","  \n","X_train = sentenize(X_train)\n","X_valid = sentenize(X_valid)\n","X_test = sentenize(test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zwNwpp99QMTx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def get_coefs(word, *arr): return word, np.asarray(arr, dtype = \"float32\")\n","embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5fiUT_ZQQU89","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["word_index = tk.word_index\n","nb_words = min(max_features, len(word_index))\n","# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n","embedding_matrix = np.zeros((nb_words, embed_size))\n","for word, i in word_index.items():\n","    if i >= max_features: continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PHa-avXkQl8B","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def build_model(rnn_units = 0, de_units = 0, lr = 0.0):\n","    encoder_inp = Input(shape = (max_text_len,), dtype = \"int32\")\n","    endcoder = Embedding(nb_words, embed_size, weights = [embedding_matrix],\n","                        input_length = max_text_len, trainable = False)(encoder_inp)\n","    endcoder = Bidirectional(CuDNNGRU(rnn_units, return_sequences = True))(endcoder)\n","    endcoder = TimeDistributed(Dense(de_units, activation = \"relu\"))(endcoder)\n","    endcoder = AttentionWeightedAverage()(endcoder)\n","    Encoder = Model(encoder_inp, endcoder)\n","\n","    decoder_inp = Input(shape = (max_sent, max_text_len), dtype = \"int32\")\n","    decoder = TimeDistributed(Encoder)(decoder_inp)\n","    decoder = Bidirectional(CuDNNGRU(rnn_units, return_sequences = True))(decoder)\n","    decoder = TimeDistributed(Dense(de_units, activation = \"relu\"))(decoder)\n","    Decoder = AttentionWeightedAverage()(decoder)\n","\n","    out = Dense(6, activation = \"sigmoid\")(Decoder)\n","    model = Model(decoder_inp, out)\n","    model.compile(loss = \"binary_crossentropy\", optimizer = Nadam(),  metrics = [\"accuracy\"])\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jCXXQKyERO_l","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model = build_model(rnn_units = 128, de_units = 64, lr = 1e-3)\n","\n","file_path = \"best_model.hdf5\"\n","ra_val = RocAucEvaluation(validation_data = (X_valid, Y_valid), interval = 1)\n","check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n","early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n","\n","history = model.fit(X_train, Y_train, batch_size = 256, epochs = 10, validation_data = (X_valid, Y_valid), \n","                    verbose = 2, callbacks = [ra_val, early_stop, check_point])\n","model = load_model(file_path, custom_objects = {\"AttentionWeightedAverage\": AttentionWeightedAverage})\n","pred = model.predict(X_test, batch_size = 1024, verbose = 2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uxcK4TB-Rcrh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["submission = pd.DataFrame()\n","submission = submission.reindex(columns = [\"id\"] + category)\n","submission[\"id\"]= test[[\"id\"]]\n","submission[category[0:6]] = pred\n","submission.to_csv(\"submission.csv\", index = False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qQyRvcY0RqA8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["submission.head()"],"execution_count":0,"outputs":[]}]}