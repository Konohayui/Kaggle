{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np, pandas as pd, os, gc, tensorflow as tf, random\nnp.random.seed(32)\nos.environ[\"PYTHONASHSEED\"] = \"64\"\nrandom.seed(128)\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads = 6, inter_op_parallelism_threads = 5)\nfrom keras import backend as K\ntf.set_random_seed(256)\nK.set_session(tf.Session(graph = tf.get_default_graph(), config = session_conf))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/donorschoose-application-screening/train.csv\")\ntest = pd.read_csv(\"../input/donorschoose-application-screening/test.csv\")\nresources = pd.read_csv(\"../input/donorschoose-application-screening/resources.csv\")\ntrain = train.sort_values(by = \"project_submitted_datetime\")\n# EMBEDDING_FILE = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nEMBEDDING_FILE = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\nembed_size = 300\ny_train = train[\"project_is_approved\"]\n\nsubmission = pd.DataFrame()\nsubmission[\"id\"] = test[[\"id\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "fb7d45ef-5ed3-4326-b349-e13932cd1696",
        "_uuid": "5d2f9436b9589542f33ab6cc7d30e69404e7e3ad",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_teachers = list(set(train.teacher_id.values))\ntest_teachers = list(set(test.teacher_id.values))\ninter = set(train_teachers).intersection(test_teachers)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "0a92acd6-1325-4d94-9b50-37e8430ee14c",
        "_uuid": "a85ef8ae04568f23393980562b8c503e5167c9f5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "char_cols = [\"project_subject_categories\", \"project_subject_categories\", \"project_title\",\n            \"project_essay_1\", \"project_essay_2\", \"project_essay_3\", \"project_essay_4\",\n            \"project_resource_summary\"]\ncat_features = [\"teacher_prefix\", \"school_state\", \"year\", \"month\", \n                \"project_grade_category\", \"project_subject_categories\", \"project_subject_subcategories\"]\nnum_features = [\"teacher_number_of_previously_posted_projects\", \"total_price_x\", \"total_price_y\", \"total_price\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "2f75e226-6d56-4f2d-9952-d60f9528d2e6",
        "_uuid": "d14014678509a674d4659576b4c137c139e29ae4",
        "trusted": false
      },
      "cell_type": "code",
      "source": "resources[\"total_price\"] = resources.quantity*resources.price\nmean_total_price = pd.DataFrame(resources.groupby(\"id\").total_price.mean())\nsum_total_price = pd.DataFrame(resources.groupby(\"id\").total_price.sum())\ncount_total_price = pd.DataFrame(resources.groupby(\"id\").total_price.count())\nmean_total_price[\"id\"] = mean_total_price.index\nsum_total_price[\"id\"] = mean_total_price.index\ncount_total_price[\"id\"] = mean_total_price.index",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "74b68037-8f46-438a-a7e5-088b73f7f2b6",
        "_uuid": "c4b9f41b8845fafaceb4d50d7c035d917f7d4dbc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def create_features(df):\n    df = pd.merge(df, mean_total_price, on = \"id\")\n    df = pd.merge(df, sum_total_price, on = \"id\")\n    df = pd.merge(df, count_total_price, on = \"id\")\n    df[\"year\"] = df.project_submitted_datetime.apply(lambda x: x.split(\"-\")[0])\n    df[\"month\"] = df.project_submitted_datetime.apply(lambda x: x.split(\"-\")[1])\n    for col in char_cols:\n        df[col] = df[col].fillna(\" \")\n    df[\"text\"] = df.apply(lambda x: \" \".join(x[col] for col in char_cols), axis=1)\n    return df\n\ntrain = create_features(train)\ntest = create_features(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "f112b674-fd97-4335-a597-94df98fee197",
        "_uuid": "16efb8db0781e156db11f857b54c6486dbc5a6a5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "cat_features_hash = [col + \"_hash\" for col in cat_features]\n\nmax_size = 15000\ndef feature_hash(df, max_size = max_size):\n    for col in cat_features:\n        df[col+\"_hash\"] = df[col].apply(lambda x: hash(x)%max_size)\n    return df\n\ntrain = feature_hash(train)\ntest = feature_hash(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "4cb26c43-657b-4301-bce2-6339d7e6362e",
        "_uuid": "2a99bf7d66aef7dfb5bc9f619ed39ebaa9f3c161",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(train, y_train, test_size = 0.1, random_state = 32)\n\ndel train, y_train; gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "874d700f-5034-46fe-a44f-411fd8de74b3",
        "_uuid": "6c86293a0ba29da30ba817ff0d27a0734ed9c565",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n#from sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing import text, sequence\nimport re\n\nmax_features = 100000#50000\nmaxlen = 300\nscaler = StandardScaler()\nX_train_num = scaler.fit_transform(X_train[num_features])\nX_valid_num = scaler.transform(X_valid[num_features])\nX_test_num = scaler.transform(test[num_features])\n\nX_train_cat = np.array(X_train[cat_features_hash], dtype = np.int)\nX_valid_cat = np.array(X_valid[cat_features_hash], dtype = np.int)\nX_test_cat = np.array(test[cat_features_hash], dtype = np.int)\ntokenizer = text.Tokenizer(num_words = max_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "16e118c4-409f-48e3-8c6e-77e15a590f3d",
        "_uuid": "982dd7b5b352f19249ebeb768c4f406cc4c868b2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def preprocess1(string):\n    '''\n    :param string:\n    :return:\n    '''\n    string = re.sub(r'(\\\")', ' ', string)\n    string = re.sub(r'(\\r)', ' ', string)\n    string = re.sub(r'(\\n)', ' ', string)\n    string = re.sub(r'(\\r\\n)', ' ', string)\n    string = re.sub(r'(\\\\)', ' ', string)\n    string = re.sub(r'\\t', ' ', string)\n    string = re.sub(r'\\:', ' ', string)\n    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n    string = re.sub(r'_', ' ', string)\n    string = re.sub(r'\\+', ' ', string)\n    string = re.sub(r'\\=', ' ', string)\n\n    return string.strip().lower()\n\nX_train[\"text\"] = X_train[\"text\"].apply(preprocess1)\nX_valid[\"text\"] = X_valid[\"text\"].apply(preprocess1)\ntest[\"text\"] = test[\"text\"].apply(preprocess1)\n\ntokenizer.fit_on_texts(X_train[\"text\"].tolist())\nlist_tokenized_train = tokenizer.texts_to_sequences(X_train[\"text\"].tolist())\nlist_tokenized_valid = tokenizer.texts_to_sequences(X_valid[\"text\"].tolist())\nlist_tokenized_test = tokenizer.texts_to_sequences(test[\"text\"].tolist())\nX_train_words = sequence.pad_sequences(list_tokenized_train, maxlen = maxlen)\nX_valid_words = sequence.pad_sequences(list_tokenized_valid, maxlen = maxlen)\nX_test_words = sequence.pad_sequences(list_tokenized_test, maxlen = maxlen)\n\ndel list_tokenized_train, list_tokenized_valid, list_tokenized_test, test; gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "e25e39cb-41ba-4a6a-91c8-9c05808b6158",
        "_uuid": "6411c94ca2cc1f4ee14d4b832bab300155c728b2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index)+1)\n# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "a4081950-6137-4665-b4e5-1865ce0d6f9a",
        "_uuid": "189a22c2367eecbc275ec7297b75b2b7aa9b5e1f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout, Conv1D, Lambda\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, SpatialDropout1D, Bidirectional, GRU, LSTM\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam, RMSprop\n\ndef build_model(units = 64, cnn_filters = 64, dr = 0.3, dense = 128, top_k = 5):\n    \n    def _top_k(x):\n        x = tf.transpose(x, [0, 2, 1])\n        k_max = tf.nn.top_k(x, k=top_k)\n        return tf.reshape(k_max[0], (-1, 2 * units * top_k))\n    \n    cat_input = Input(shape = (len(cat_features_hash), ))\n    num_input = Input(shape = (len(num_features), ))\n    word_input = Input(shape = (maxlen, ))\n    \n    cat_emb = Embedding(max_size, 10)(cat_input)\n    x_cat = SpatialDropout1D(dr)(cat_emb)\n    x_cat = Flatten()(x_cat)\n    \n    word_emb = Embedding(nb_words, embed_size, weights = [embedding_matrix],\n                        input_length = maxlen, trainable = False)(word_input)\n    x_word = SpatialDropout1D(dr)(word_emb)\n    x_word = Bidirectional(LSTM(units, return_sequences = True))(x_word)\n    x_word = Bidirectional(GRU(units, return_sequences = True))(x_word)\n#     x_word = Conv1D(cnn_filters, kernel_size = 3, activation = \"relu\")(x_word)\n#     x_word = GlobalMaxPooling1D()(x_word)\n    \n    k_max = Lambda(_top_k)(x_word)\n    avg_pool = GlobalAveragePooling1D()(x_word)\n    x_word = concatenate([k_max, avg_pool])\n    \n    x_cat = Dense(dense, activation = \"relu\")(x_cat)\n    x_num = Dense(dense, activation = \"relu\")(num_input)\n    \n    out_put = concatenate([x_cat, x_num, x_word])\n#     out_put = Dropout(dr)(Dense(dense//2, activation = \"relu\")(out_put))\n    out_put = Dense(1, activation = \"sigmoid\")(out_put)\n    model = Model(inputs = [cat_input, num_input, word_input], outputs = out_put)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = 1e-3), metrics = [\"accuracy\"])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "_cell_guid": "0e5f5f3c-ad00-49a6-888c-7f4220411502",
        "_uuid": "8cb21e61f166cbc9dd9089c4bb11a64a604187aa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint, EarlyStopping\nmodel = build_model()\nfile_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\",\n                              save_best_only = True, verbose = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3, verbose = 1)\n\nmodel.fit([X_train_cat, X_train_num, X_train_words], Y_train, epochs = , batch_size = 256,\n          validation_data = ([X_valid_cat, X_valid_num, X_valid_words], Y_valid),\n          verbose = 1, callbacks = [check_point, early_stop])\n\nmodel = load_model(file_path, custom_objects = {\"tf\": tf})\nprediction = model.predict([X_test_cat, X_test_num, X_test_words], batch_size = 2048, verbose = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "77b8775a-ebb4-490f-97a7-756df9213274",
        "_uuid": "5bef2f600e61d644558ff474116a202a96ba7481",
        "trusted": false
      },
      "cell_type": "code",
      "source": "submission[\"project_is_approved\"] = prediction\nsubmission.to_csv(\"submission.csv\", index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "24517ff0-f64b-421f-a375-8d2723440598",
        "_uuid": "c06a491094c880910420f38c85a571594418cbde",
        "trusted": false
      },
      "cell_type": "code",
      "source": "submission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "489f7b48-d885-420b-adf8-5f43fefe1de6",
        "_uuid": "237ebcff3d062dd2ab290892033dd95eef2cb84d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}